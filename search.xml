<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[how to write resilient mongodb applications]]></title>
    <url>%2Fhow-to-write-resilient-mongodb-applications%2F</url>
    <content type="text"><![CDATA[前言：本文翻译自MongoDB团队工程师A. Jesse Jiryu Davis的博客文章。文章描述了在遇到错误时如何安全的进行重试。虽然这项技术已被MongoDB 3.6的新内置功能Retryable Writes取代。 可重写的写操作比这里描述的技术简单得多。但是作者解决问题的方式还是能给我们一些启发。 有一次，在2012年初的一个寒冬午后，我遇到了一位非常生气的MongoDB客户。 他来到我们的“MongoDB Office Hours”办公室，他有一个问题：“我怎样才能使我的应用程序在网络错误，中断和其他异常情况下具有弹性？我可以每次重试操作直到成功？“他要求知道为什么我们没有发布一个适用于所有应用程序的简单明智的策略。 这个人，我会叫他Ian，很不高兴，我也帮不了他。我的内疚已经让我忘记了那天的细节。我们坐在一间没有窗户的房间里，这是我们唯一可以在我们的小办公室聊天的自由空间。这里有一个讨厌的荧光灯。我们并排坐在桌子的边缘，因为房间没有椅子。在Ian的眼睛下有深深的黑眼圈，就像他已经为这个问题担心了一整晚。 “我如何编写弹性代码？” 我唯一可以告诉Ian的是，“我们无法发布一个策略来处理网络错误和中断，并为您指出错误，因为我们不知道您应用程序的细节。您可以选择很多不同的操作方式，在延迟和可靠性之间做出折中，以及在一个操作肯能被执行两次的风险与完全不做这两者之间进行权衡，这就是为什么我们没有尝试写出一种“一刀切”的策略。就算可以，但是不同语言下驱动的行为不同，我们必须为每种语言发布指南。“ 我对我的回答并不满意，Ian也一样。 在此后的几年中，我努力想出了一个更好的答案。首先我写了服务发现和监测规范，我们所有的驱动程序现在已经实现了。该规范极大地提高了驱动程序在面对网络错误和服务器故障时的鲁棒性。像Ian这样的投诉现在比较少见，因为驱动很少会抛出异常。此外，所有驱动程序现在都表现相同，我们使用标准化的通用测试套件验证其行为。 其次，我开发了一种名为‘Black Pipe’的测试技术，因此Ian可以测试他的代码在与MongoDB交互时如何响应网络故障，命令错误或任何其他事件。‘Black Pipe’使用方便且准确;它使得错误情况可重现且易于测试。 现在可以回答Ian了。如果他今天来到我们在时代广场的大办公室，你会如何回答他？编写弹性MongoDB应用程序的策略是什么？ 我们会告诉Ian，如何做到这一点： 123updateOne(&#123;'_id': '2016-06-28'&#125;, &#123;'$inc': &#123;'counter': 1&#125;&#125;, upsert=True) 该操作通过在_id为今天日期的文档中递增名为“counter”的字段来计数事件发生的次数。如果这是今天的第一个事件，参数“upsert = True”告诉MongoDB创建文档。 什么会出错瞬态错误当Ian将他的updateOne消息发送到MongoDB时，驱动程序可能会看到来自网络层的暂时错误，例如TCP重置或超时。 瞬态网络错误，故障转移或降级驱动不知道服务器是否收到消息，所以Ian不知道他的计数器是否增加。 还有其他瞬态错误看起来与网络暂时性问题相同。如果primary节点出现故障，下次尝试向其发送消息时，驱动程序会收到网络错误。这个错误很简短，因为副本集会在几秒钟内选出一个新的primary节点。同样，如果primary服务器退出（它仍然正常工作，但已经辞去其primary角色），它将关闭所有连接。下次驱动程序向服务器发送消息时，它认为它是primary节点，它会从服务器收到网络错误或“not master”回复。 在所有情况下，驱动程序都会向Ian的应用程序抛出连接错误。 持续的错误也可能存在持续的网络中断。当驱动程序首次检测到此问题时，它看起来像一个暂时性问题：驱动程序发送一条消息，但是无法读取响应。 Persistent Network Outage 持续的网络中断Ian再一次无法分辨服务器是否接收到消息并递增计数器。 网路中断与网络暂时性问题的区别是Ian试图再次操作只会得到另一个网络错误。但是只有他尝试后才知道。 命令错误当驱动程序发送消息时，MongoDB可能会返回一个特定的错误，表示已收到命令但无法执行。也许命令格式不正确，服务器磁盘空间不足，或Ian的应用程序未被授权。 所以，有三个不能完全可区分的错误，需要Ian的代码有不同回应。你会给Ian什么样的策略来使他的应用具有弹性？ 是因为我们没有事务吗？您可能想知道这是否是只有MongoDB才有的问题，因为它没有事务。假设Ian使用了传统的SQL服务器。他打开一个事务，更新一行，并发送COMMIT消息。然后有一个网络临时性错误。他无法从服务器读取确认消息。他知道交易是否已经实施吗？ 保证操作只执行一次，使用SQL服务器与使用非事务性服务器（如MongoDB）面临的问题相同。 MongoDB驱动程序如何处理错误？为了制定您的策略，您需要知道MongoDB驱动程序自己如何响应不同类型的错误。 服务器发现和监测规范要求MongoDB驱动程序跟踪每个连接到的服务器的状态。例如，它可能是一个3节点副本集，如下所示： Server 1: Primary Server 2: Secondary Server 3: Secondary 这个数据结构被称为“拓扑描述”。如果在与服务器交谈时出现网络错误，驱动程序会将该服务器的类型设置为“未知”，然后引发异常。现在拓扑描述是： Server 1: Unknown Server 2: Secondary Server 3: Secondary Ian尝试的操作不会自动重试; 然而，下一个操作会被阻塞，因为驱动程序正在重新发现主驱动器。它每秒重新检查每台服务器两次，最多持续30秒，直到重新连接到主服务器或检测到新主服务器被选中。现在MongoDB的选举只需要一两秒钟，然后驱动程序在此之后大约半秒会收到选举结果。 另一方面，如果网络持续中断，则在30秒之后，驱动程序会抛出“server selection timeout”。 在命令错误的情况下，驱动程序对服务器的看法没有改变：如果服务器是primary或secondary，它仍然是。因此，驱动程序不会在拓扑描述中更改服务器的状态，只会引发异常。 （要了解有关服务器发现和监测规范的更多信息，请阅读我的文章PyMongo，Perl和C中的服务器发现和监控，或观看MongoDB World 2015上的演讲，MongoDB驱动程序和高可用性：深度分析。在那里我详细讨论了规范中描述的数据结构以及规范如何告诉驱动程序对错误做出反应，这是本文关于弹性应用程序的一个很好的背景。） 错误的重试策略我见过一些。 不要重试默认是根本不重试。这种策略在遇到前面描述的三种错误情况时可能会失败。 瞬态网络错误 持续中断 命令错误 可能计数丢失 正确 正确 在发生瞬时网络错误的情况下，Ian将消息发送到服务器，并且不知道服务器是否收到它。如果没有，该事件从不计算在内。他的代码可能会记录错误，然后继续前进。 有趣的是，面对长期的网络中断或命令错误，“不要重试”是正确的策略，因为这些是非瞬态错误，无法通过重试改善。 总是重试一些程序员编写代码重试任何失败的操作五次，或者，如果他们真的关心弹性，十次。我在很多生产应用程序中都看到了这一点。 123456789i = 0while True: try: do_operation() break except network error: i += 1 if i == MAX_RETRY_COUNT: throw 去年，我与一位名为Sam的Rackspace工程师进行了交谈。他接手了一个应用了这个不好的策略的Python代码库：它只要接收到任何异常就会一遍又一遍地重试。 Sam认为这是愚蠢的。他注意到我最近根据服务器发现和监测规范重新编写了PyMongo的客户端代码，他推测他可以更好地利用更强大的新驱动程序。在那里有一个更聪明的重试策略，但他不知道它到底是什么。事实上，正是从我与Sam的对话中，这篇文章诞生了。 Sam看到了什么？为什么Ian不应该重试每次操作五到十次？ 瞬态网络错误 持续中断 命令错误 可能过多计数 浪费时间 浪费时间 在网络出现瞬态错误的情况下，Ian不会丢失计数，现在他可能会计数过多。因为如果服务器在发生网络错误之前先读取其第一条updateOne消息，则第二条updateOne消息会再次递增计数器。 另一方面，在持续的网络中断期间，多次重试是浪费时间。第一次网络错误后，驱动程序将primary服务器标记为“unknown”; 当Ian重试该操作时，它会在驱动程序尝试重新连接时阻塞，每秒检查两次，持续30秒。如果驱动程序代码中的所有努力都没有成功，那么再次尝试进入驱动程序的重试循环是徒劳的。这会导致他的应用程序出现排队和延迟。 命令错误也是如此：如果Ian的应用程序未被授权，则重试五次不会改变该情况。 重试一次网络错误现在我们比较接近一个明智的策略。在Ian初始操作失败并出现网络错误后，Ian不知道错误是暂时的还是持久的，因此他只重试一次操作。单次重试进入驱动程序的30秒重试循环。如果网络错误持续30秒，则可能会持续更长时间，所以Ian放弃了继续重试。 但是，面对命令错误，他根本不会重试。 瞬态网络错误 持续中断 命令错误 可能过多计数 正确 正确 所以我们只剩下一种情况还有问题 - Ian怎么避免过多计数的可能性？ 重试网络错误，使操作幂等幂等性操作无论是一次还是多次执行都具有相同的结果。如果Ian的所有操作都是幂等的，那么他可以安全地重试它们，而不会发生过多计数或任何由于发送消息两次带来的其他类型错误。 瞬态网络错误 持续中断 命令错误 正确 正确 正确 那么Ian应该如何让他的updateOne操作幂等？ 幂等操作MongoDB有四种操作：查找，插入，删除和更新。前三个很容易幂等性; 让我们先来处理它们。 Find查询自然是幂等的。 1234try: doc = findOne()except network err: doc = findOne() 两次检索文档与检索一次文档结果一样。 Insert比查询困难一点，但不是太糟糕。 123456789doc = &#123;_id: ObjectId(), ...&#125;try: insertOne(doc)except network err: try: insertOne(doc) except DuplicateKeyError: pass # first try worked throw 这个伪代码的第一步是在客户端生成一个唯一的ID。 MongoDB的ObjectId是为这种用法而设计的，但任何独特的值都可以。 现在，Ian试图插入文档。如果因网络错误而失败，他会再次尝试。如果第二次尝试失败且服务器出现重复键错误，则第一次尝试成功，但由于网络层发生错误，他无法读取服务器响应。如果出现其他问题，他取消操作并抛出异常。 这个插入是幂等的。唯一需要警告的是，我假设Ian除了MongoDB自动创建的_id之外，没有唯一索引。如果还有其他唯一索引，那么他必须解析重复键错误。 Delete如果Ian删除一个文档时指定一个唯一的key，那么执行两次与执行一次相同。 1234try: deleteOne(&#123;'key': uniqueValue&#125;)except network err: deleteOne(&#123;'key': uniqueValue&#125;) 如果第一个被执行，但是Ian得到一个网络异常并再次尝试，那么第二个删除就是空操作;它只是不匹配任何文档。该删除可以安全地重试。 同时删除许多文件甚至更容易： 1234try: deleteMany(&#123;...&#125;)except network err: deleteMany(&#123;...&#125;) 如果Ian删除所有匹配过滤器的文档，并且他的代码出现网络错误，那么他可以再次安全地尝试。无论是deleteOne运行一次还是两次，结果都是一样的：所有匹配的文档都被删除。 在这两种情况下都存在竞争条件：另一个进程可能会在两次尝试删除它们之间插入新的匹配文档。但是这场比赛并不比他的代码没有重试更糟糕。 Update那么更新呢？首先考虑那种自然而然是幂等的更新，让我们放松一下。 12345# Idempotent update.updateOne(&#123; '_id': '2016-06-28'&#125;, &#123;'$set':&#123;'sunny': True&#125;&#125;, upsert=True) 这与我们原来的例子不一样; Ian并没有增加一个变量的值，他把今天的“sunny”字段设置为真，让自己振作起来。说两次今天是阳光明媚的，就像说一次一样好。在这种情况下，updateOne可以安全地重试。 一般原则是有一些MongoDB更新运算符是幂等的，有些则不是。 $set是幂等的，因为如果两次将某些值设置为相同的值，则不会有任何区别，而$inc不是幂等的。 如果Ian的更新操作符是幂等的，那么他可以很容易地重试它：他尝试一次，如果他得到网络异常，他会再次尝试。 123456try: updateOne(&#123;' _id': '2016-06-28'&#125;, &#123;'$set':&#123;'sunny': True&#125;&#125;, upsert=True)except network err: try again, if that fails throw 所以现在我们终于准备好解决我们最难的例子，原始的非幂等updateOne： 123updateOne(&#123; '_id': '2016-06-28'&#125;, &#123;'$inc': &#123;'counter': 1&#125;&#125;, upsert=True) 如果Ian偶然做了两次这样的操作，他会将计数增加2。 我们如何将它变成幂等操作？我们将分成两步。每一步都是幂等的，通过将它转换成一对幂等运算，我们将安全地重试。 我们假设一开始文档的counter值是N： 1234&#123; _id: '2016-06-28', counter: N&#125; 在第一步中，Ian先不管counter当前的值，他只是给一个“pending”数组添加一个标记。这里他需要一些独特的东西; 一个ObjectId是很好的选择： 1234567oid = ObjectId()try: updateOne(&#123; '_id': '2016-06-28'&#125;, &#123;'$addToSet': &#123;'pending': oid&#125;&#125;, upsert=True)except network err: try again, then throw $addToSet是幂等运算符之一。如果它运行两次，令牌只会添加一次到数组中。现在文档看起来像这样： 12345&#123; _id: '2016-06-28', counter: N, pending: [ ObjectId("...") ]&#125; 第二步，对于单个操作，Ian通过_id及其待处理标记查询文档，删除待处理标记并增加计数器。 123456789try: # Search for the document by _id and pending token. updateOne(&#123;'_id': '2016-06-28', 'pending': oid&#125;, &#123;'$pull': &#123;'pending': oid&#125;, '$inc': &#123;'counter': 1&#125;&#125;, upsert=False)except network err: try again, then throw 所有的MongoDB更新，无论它们是否是幂等的，都是原子的：单个updateOne完全成功或根本没有任何作用。所以只有当计数器增加1时, 令牌才会从待处理数组中移除。 总的来说，这个更新是幂等的。想象一下，Ian将令牌移出数组并在第一次尝试时递增计数器，但是之后他无法读取服务器响应，因为存在网络错误。他第二次尝试是无效的，因为查询要求匹配具有待处理令牌的文档，但他已经将其取出。 所以Ian可以安全地重试这​​个updateOne。无论它执行了一次还是两次，文档结果都是一样的： 12345&#123; _id: '2016-06-28', counter: N + 1, pending: [ ]&#125; 任务完成？现在你已经做到了：你重现实现了Ian原来的updateOne，这是不安全的重试，并通过将它分成两步来使它变得幂等。 这项技术有几点需要注意。其中之一是Ian简单的$inc操作现在需要两次往返。它需要延迟两倍，负载加倍。如果在网络瞬态错误中被少计入一次或超过一次没有什么问题，他就不应该使用这种技术。 另一个警告是，他需要一个每晚执行的清理过程。考虑一下：如果第二步从未完成，会发生什么？ 12345678try: updateOne(&#123;'_id': '2016-06-28', 'pending': oid&#125;, &#123;'$pull': &#123;'pending': oid&#125;, '$inc': &#123;'counter': 1&#125;&#125;, upsert=False)except network err: try again, then throw 假设Ian在添加待处理令牌后，将其移除并增加计数器之前，网络中断开始。该文件处于这种状态： 12345&#123; _id: '2016-06-28', counter: N, pending: [ ObjectId("...") ]&#125; Ian需要一个每晚执行的任务来查找今天中止的任务遗留的任何待处理令牌，并完成更新计数器。为了避免并发问题，他一直等到一天结束。他的任务使用聚合管道来查找具有待处理令牌的文档，并将其数量添加到当前计数中： 1234567891011121314151617181920pipeline = [&#123; '$match': &#123;'pending.0': &#123;'$exists': True&#125;&#125;&#125;, &#123; '$project': &#123; 'counter': &#123; '$add': [ '$counter', &#123;'$size': '$pending'&#125; ] &#125; &#125;&#125;]for doc in collection.aggregate(pipeline): collection.updateOne( &#123; '_id': doc._id&#125;, &#123; '$set': &#123;'counter': doc.counter&#125;, '$unset': &#123;'pending': True&#125; &#125;) 对于每个聚合操作结果，此任务使用最终计数器值更新源文档，并清除待处理数组。 updateOne可以安全地重试，因为它使用幂等运算符$set和$unset。Ian的清理任务可以一直尝试，直到它成功，无论他的网络有多么糟糕。执行此清理任务后，Ian今天的事件数量最终将是正确的。 如果Ian接受这些缺点 - 一次增加需要两次往返，并且可能在一天结束之前不能完成 - 那么对于非常关键的操作，这种技术是一种可靠的方式来使其计数器只增加一次。 Ref How To Write Resilient MongoDB Applications Retryable Writes]]></content>
  </entry>
  <entry>
    <title><![CDATA[Deadlock caused by forking a multithreaded process]]></title>
    <url>%2Fdeadlock-caused-by-forking-a-multithreaded-process%2F</url>
    <content type="text"><![CDATA[最近生产环境的一个模块突然卡住了，模块的作者已经去其他组了，没办法只能自己查问题了🤣。 该模块使用python的logging标准库记录日志，自定义kafka作为handler，日志通过kafka发送到Elasticsearch，然后就是ELK日志处理了。 模块为了提高吞吐，内部使用了多进程，问题恰恰就出现在了这里。 使用GDB attach到卡死的进程，执行py-bt命令，输出如下： Traceback (most recent call first): File “/usr/lib64/python2.7/multiprocessing/forking.py”, line 135, in poll pid, sts = os.waitpid(self.pid, flag) File “/usr/lib64/python2.7/multiprocessing/forking.py”, line 154, in wait return self.poll(0) File “/usr/lib64/python2.7/multiprocessing/process.py”, line 145, in join res = self._popen.wait(timeout) File “parser/parser_main.py”, line 76, in data_parse process.join() 主进程里使用multiprocessing模块创建多进程执行任务，这里主进程在等待子进程执行结束。 阅读multiprocessing模块的源代码，可以发现其创建子进程时调用了os.fork()函数，该函数是python os模块对系统调用fork的封装。 子进程的输出： Traceback (most recent call first): Waiting for the GIL File “sdk/python-lib/kafka/producer/record_accumulator.py”, line 223, in append with self._tp_locks[tp]: File “sdk/python-lib/kafka/producer/kafka.py”, line 516, in send self.config[‘max_block_ms’]) File “sdk/python-lib/elk_logging/handlers/kafka.py”, line 37, in emit self.format(record).encode(‘utf-8’)) elk_logging模块自定义了kafka handler，模块导入时通过logging.config.dictConfig()配置logging模块。kafak发送消息时获取不到锁self._tp_lock[tp]，然后一直等待。 在Google上搜索“kafka self._tp_lock deadlock”没有发现有价值的信息，基本排除是kafka库的bug，如果是kafka库的bug，网上不太可能一点线索也没有。 前文我们提到这里使用了fork创建子进程，当内核执行fork时，生成的子进程是父进程的副本，子进程获得父进程的数据空间、堆和栈的副本（这里是子进程自己拥有的副本，并不与父进程共享这些存储空间。很多实现使用了写时复制技术，一开始这些区域被父子进程共享，如果任意一个进程修改这些区域，则内核只为修改的那块内存制作一个副本，通常是虚拟内存的一页）。 在Linux系统中调用fork，创建的子进程中只有一个线程——父进程中调用fork函数的线程。 Note the following further points: The child process is created with a single thread—the one that called fork(). The entire virtual address space of the parent is replicated in the child, including the states of mutexes, condition variables, and other pthreads objects; the use of pthread_atfork(3) may be helpful for dealing with problems that this can cause. 也不是所有的系统都是这样，Solaris系统libthread库提供的fork函数会“复制“所有线程。 Solaris libthread supports both fork() and fork1(). The fork() call has “fork-all” semantics–it duplicates everything in the process, including threads and LWPs, creating a true clone of the parent. The fork1() call creates a clone that has only one thread; the process state and address space are duplicated, but only the calling thread is cloned. POSIX libpthread supports only fork(), which has the same semantics as fork1() in Solaris threads. POSIX标准定义的行为是fork只“复制”一个线程。 那么这些与上面的死锁有什么关系？python标准库logging模块在创建logger时会调用到以下代码，可以看到一个logger只会实例化一次，对应的handler和formatter等也只会实例化一次。 logging/__init__.py manager.getLogger(name)123456789101112131415161718_acquireLock()try: if name in self.loggerDict: rv = self.loggerDict[name] if isinstance(rv, PlaceHolder): ph = rv rv = (self.loggerClass or _loggerClass)(name) rv.manager = self self.loggerDict[name] = rv self._fixupChildren(ph, rv) self._fixupParents(rv) else: rv = (self.loggerClass or _loggerClass)(name) rv.manager = self self.loggerDict[name] = rv self._fixupParents(rv)finally: _releaseLock() 如果自定义了handler，配置logging模块时会导入指定的handler class，并实例化。 logging/config.py DictConfigurator.configure_handler()123456789101112131415161718if '()' in config: c = config.pop('()') if not callable(c): c = self.resolve(c) factory = celse: cname = config.pop('class') klass = self.resolve(cname) factory = klassprops = config.pop('.', None)kwargs = dict([(k, config[k]) for k in config if valid_ident(k)])try: result = factory(**kwargs)except TypeError as te: if "'stream'" not in str(te): raise kwargs['strm'] = kwargs.pop('stream') result = factory(**kwargs) 父进程和fork出来的子进程使用的时相同的对象实例，相同的logger，相同的kafka handler。 kafka producer发送消息是多线程异步的。KafkaProducer实例化时会启动一个消息发送线程。有消息过来后，先通过RecordAccumulator类实例将消息追加到内部list，然后唤醒self._sender发送线程，发送线程通过RecordAccumulator类实例检查哪些可以发送，整个过程RecordAccumulator类实例维护了topic+partition相关的锁。1234567891011121314def __init__(self, **configs): ... self._accumulator = RecordAccumulator(message_version=message_version, metrics=self._metrics, **self.config) self._sender = Sender(client, self._metadata, self._accumulator, self._metrics, guarantee_message_order=guarantee_message_order, **self.config) self._sender.daemon = True self._sender.start() self._closed = False self._cleanup = self._cleanup_factory() atexit.register(self._cleanup) log.debug("Kafka producer started") 如果fork时RecordAccumulator类实例中某个topic+partition的锁还没有释放，子进程中该锁永远不会被释放，因为没有了发送线程，如果子进程中恰好向同一个topic和同一个partition发送消息就会死锁。 由于子进程不会常驻在后台，所以满足上述条件还是比较难的，从表现上来看，5个月里发生了一次死锁。 后记避免这种问题，最好的办法就是不要fork一个多线程进程。 python的标准库logging模块曾经因为类似的原因出过问题。现在官方文档已经指出： Note that safely forking a multithreaded process is problematic. Google甚至还创建过一个项目python-atfork，模仿pthread_atfork，可以注册函数，保正多线程的情况下可以安全的fork。当然，该项目已经被废弃了。 如果使用较新的python版本（3.5+ ？）可以指定multiprocessing模块创建进程的方式，可以尝试spawn方式。 Ref UNIX环境高级编程 Linux fork manual Special Issues for fork() and Solaris Threads python multiprocessing doc Safe to call multiprocessing from a thread in Python？ reentrancy, thread safe, asynchronous signal safe, interrupt safe and cancellation safe Threads and fork(): think twice before mixing them]]></content>
  </entry>
  <entry>
    <title><![CDATA[Debug Python with GDB in off-line CentOS]]></title>
    <url>%2Fdebug-python-with-gdb-in-offline-centos%2F</url>
    <content type="text"><![CDATA[GDB不仅可以调试C/C++程序，它还可以调试Python等程序，是非常重要和有力的开发工具之一。大部分情况下，都可以直接安装发行版提供的gdb来调试程序，但是，某些情况下需要自己编译，例如，机器无法上网；甚至没有权限修改系统，无法安装软件。下面以CentOS为例，介绍如何在离线并且没有全部权限的情况下使用GDB调试Python程序。 首先，需要在相同系统版本可以上网的机器上编译GDB wget -c http://ftp.gnu.org/gnu/gdb/gdb-8.0.1.tar.gz tar xvf gdb-8.0.1.tar.gz cd gdb-8.0.1.tar.gz mkdir build cd build ../configure --enable-static --with-python --disable-interprocess-agent --with-lzma make 需要拿到离线机器使用，所以这里使用‘–enable-static –disable-interprocess-agent’，期望获得一个静态链接的二进制文件。通过‘–with-python’启用了python调试功能。‘–with-lzma’启用了lzma支持，由于某些发行版分发的二进制文件使用了‘MiniDebugInfo’特性：在二进制文件中存在一个名为‘gnu_debugdata’的特殊区域，该区域保存了经过lzma压缩的额外符号信息。 如果在configure过程中报错，一般都是缺少一些依赖包，根据提示进行安装就可以了。 编译完成后，需要的文件都在build/gdb目录内，将他们复制到另一个目录 mkdir -p ~/gdb/data-directory cd gdb cp gdb ~/gdb/gdb cp -r data-directory/* ~/gdb/data-directory cd ~/gdb chmod +x ./gdb data-directory目录内保存了gdb需要使用的数据文件。现在就可以使用这个gdb在离线系统上简单调试程序了 ./gdb --data-directory=./data-directory your-program 但是当进入到一些库函数时，会出现错误，因为系统上没有相应的调试信息文件。需要手动部署这些文件，以调试Python为例，首先获取系统中python包的版本 yum list installed | grep ^python.x86_64 假如得到的信息是‘python.x86_64 2.7.5-58.el7’，去debuginfo.centos.org下载相应的调试信息文件，例如，’python-debuginfo-2.7.5-58.el7.x86_64.rpm‘。 将该文件解包 rpm2cpio python-debuginfo-2.7.5-58.el7.x86_64.rpm | cpio -idmv 正常情况下，该文件是通过rpm安装到系统里的，安装后的结构和直接解压出来的结构可能不同，系统目录： ls -l /usr/lib/debug/ drwxr-xr-x. 3 root root 64 11月 5 2016 . dr-xr-xr-x. 44 root root 4096 10月 28 21:38 .. lrwxrwxrwx. 1 root root 7 10月 28 21:21 bin -&gt; usr/bin lrwxrwxrwx. 1 root root 7 10月 28 21:21 lib -&gt; usr/lib lrwxrwxrwx. 1 root root 9 10月 28 21:21 lib64 -&gt; usr/lib64 lrwxrwxrwx. 1 root root 8 10月 28 21:21 sbin -&gt; usr/sbin drwxr-xr-x. 6 root root 65 10月 28 21:21 usr 解包目录： ls -al ./usr/lib/debug/ drwxr-xr-x. 5 buildbot buildbot 46 10月 29 23:26 . drwxrwxr-x. 3 buildbot buildbot 19 10月 29 23:26 .. drwxr-xr-x. 114 buildbot buildbot 4096 10月 29 23:26 .build-id drwxr-xr-x. 2 buildbot buildbot 40 10月 29 23:26 .dwz drwxr-xr-x. 4 buildbot buildbot 30 10月 29 23:26 usr 系统目录中的符号链接，解包目录里是没有的，需要手动加上。解包目录里的‘.build-id’文件夹是非常重要的文件夹。 GDB允许将调试信息和可执行代码分开存放，因为一般情况，调试信息比可执行代码大很多，普通用户不需要调试信息。分开存放很好的解决了此问题。 GDB支持两种指定调试信息文件的方法： 可执行文件包含一个‘debug link’，其指定了调试文件的名字。通常调试信息文件的名字是‘executable.debug’，其中，‘executable’是可执行文件的名字（不包括路径）。并且，‘debug link’还包含了可执行文件的CRC32值，GDB可以利用该值判断可执行文件和调试信息文件是否来自相同的build。 可执行文件包含一个特殊的字符串‘build-id’，该字符串同时也存在于调试信息文件中。调试信息文件的名字可以通过‘build-id’计算出来。 对于以上两种不同的方式，GDB采用两种方法查找调试信息文件： 对于‘debug link’方式，GDB先在可执行文件所在目录搜索指定名字的调试信息文件，然后是可执行文件所在目录中名为‘.debug’的子目录里搜索，最后，在所有全局‘debug directory’里的和可执行文件绝对目录名字一样的子目录里搜索。 对于‘build-id’方式，GDB在每一个全局‘debug directory’里名为‘build-id’的子目录中搜索名为‘nn/nnnnnnnn.debug’的文件（CentOS中该文件是一个符号链接）。 准备好了以上文件后，还需要最后一个很重要的文件，该文件定义了调试需要用到的GDB命令。该文件位于 ./usr/lib/debug/usr/lib64/libpython2.7.so.1.0.debug-gdb.py 将该文件重命名为python-gdb.py，并和其他文件放一起。现在gdb目录的结构是这样的： gdb data-directory usr python-gdb.py 启动gdb后需要import python-gdb.py，修改PYTHONPATH export PYTHONPATH=&quot;${PYTHONPATH}:${HOME}/gdb&quot; 然后启动gdb ./gdb --data-directory=./data-directory 设置debug文件查找目录，告诉GDB去准备好的解包目录找调试信息 set debug-file-directory ./usr/lib/debug/ 载入python模块，载入自定义GDB命令 python import python-gdb 然后attach到需要调试的进程就可以了 attach pid Ref MiniDebugInfo Building Python Statically how-i-can-static-build-gdb-from-source How do I extract the contents of an rpm? Debugging Information in Separate Files]]></content>
  </entry>
  <entry>
    <title><![CDATA[MongoDB Data Migration]]></title>
    <url>%2FMongoDB-Data-Migrate%2F</url>
    <content type="text"><![CDATA[MongoDB使用json和bson（binary json）格式存储信息。json使用起来非常方便，但是其并不支持bson里的所有数据类型。如果使用json备份数据就会有信息丢失。 备份和恢复MongoDB数据库可能会消耗大量的CPU、内存和磁盘资源，最好是在非高峰期进行。 需要考虑数据一致性的问题。 一致性（Consistency）一致性是指事务使得系统从一个一致的状态转换到另一个一致状态。事务的一致性决定了一个系统设计和实现的复杂度。事务可以不同程度的一致性：强一致性：读操作可以立即读到提交的更新操作。弱一致性：提交的更新操作，不一定立即会被读操作读到，此种情况会存在一个不一致窗口，指的是读操作可以读到最新值的一段时间。最终一致性：是弱一致性的特例。事务更新一份数据，最终一致性保证在没有其他事务更新同样的值的话，最终所有的事务都会读到之前事务更新的最新值。如果没有错误发生，不一致窗口的大小依赖于：通信延迟，系统负载等。 其他一致性变体还有：单调一致性：如果一个进程已经读到一个值，那么后续不会读到更早的值。会话一致性：保证客户端和服务器交互的会话过程中，读操作可以读到更新操作后的最新值。 copydb &amp; clone Concurrency copydb and clone do not produce point-in-time snapshots of the source database. Write traffic to the source or destination database during the copy process will result in divergent data sets. copydb does not lock the destination server during its operation, so the copy will occasionally yield to allow other operations to complete. clone does not snapshot the database. If any clients update the database you’re copying at any point during the clone operation, the resulting database may be inconsistent. 有数据一致性的问题。 mongodump &amp; mongorestore 以bson格式保存，恢复完成后必须重建索引。 对于小型的数据库是简单高效的备份恢复工具，但是对于大型系统不是一个理想的选择。 mongodump会显著影响MongoDB的性能。如果数据库大小超过系统内存，有可能出现内存不足和页错误。 without –oplog， if there are write operations during the dump operation, the dump will not reflect a single moment in time. Changes made to the database during the update process can affect the output of the backup. 使用oplog可以保证数据一致性 master &amp; slaveslave把master的数据保存在了local.sources collection。迁移完成后需要进一步处理。本质上也是利用了oplog, 可以保证数据一致性。 Replication Set相当于增强的master slave模式，带有failover机制，多个节点选取一个为主节点，其他为次节点，主节点可读写，次节点只能读，当主节点挂掉会自动选举新的主节点，方便加入新的次节点，扩展数据库系统，本质上也是利用了oplog, 可以保证数据一致性。 总结copydb &amp; clone 有数据一致性问题不考虑使用。master &amp; slave模式，slave后续还需要进一步操作，并且官方已不推荐部署此模式。Replication Set提供了很多高级特性，是官方现在推荐的部署模式。处理数据迁移时，master &amp; slave和Replication Set比较接近，前者需要进一步处理，后者配置略麻烦，都相当于自动mongodump + oplogreplay，在数据不是很大的情况下，可以配置一个单节点的Replication Set配合mongodump with oplog比较合适。 oplog有一个非常重要的特性——幂等性（idempotent）。即对一个数据集合，使用oplog中记录的操作重放时，无论被重放多少次，其结果是一样的。举例来说，如果oplog中记录的是一个插入操作，并不会因为你重放了两次，数据库中就得到两条相同的记录。 还有一种方法是使用文件系统提供的快照功能，比如LVM。 RefHow To Back Up, Restore, and Migrate a MongoDB Database on Ubuntu 14.04 Migrating MongoDB instances with no down-time MongoDB Clone MongoDB Copydb MongoDB Replication Master Slave Replication]]></content>
  </entry>
  <entry>
    <title><![CDATA[Inverted Index in the Luence]]></title>
    <url>%2FInverted-Index-in-the-Luence%2F</url>
    <content type="text"><![CDATA[假设有一本书，前面有目录，中间是正文，最后会有附录。有这样一种附录，它会把正文中出现的关键名词列出来，并附上正文中出现的页码。当我们想找某个术语的相关信息时，去附录里查正文出现的页码显然方便些。目录类似于我们常规的索引，一个章节/文件里有什么内容；附录保存的信息和它相反，这个信息出现在哪些章节/文件里。我们称附录这种数据的索引方式叫做“Inverted Index”，中文一般翻译为“倒排索引”。 这种数据结构是搜索程序的核心数据结构，它本身并不复杂，很容易理解，网上的解释也很多，具体可以参考维基百科。因为最近接触ElasticSearch，比较好奇它的倒排索引是怎么实现的。 ElasticSearch是一个基于Lucene的全文搜索引擎。Lucene提供了搜索的核心功能，维护倒排索引是其中的功能之一。 Luence的倒排索引是由许多子片段组成的。每一个子片段是一个完整的能独立工作的子索引，由几个二进制文件组成。当有新的文档需要被索引时，会新建子索引，当有文档被删除时，并不会删除对应的索引数据，而是做标记，记录这些数据已经失效了。子索引数据的合并和删除会在某一时间统一执行。 Luence里对数据的基本定义是这样的： The fundamental concepts in Lucene are index, document, field and term. An index contains a sequence of documents. A document is a sequence of fields. A field is a named sequence of terms. A term is a sequence of bytes. The same sequence of bytes in two different fields is considered a different term. Thus terms are represented as a pair: the string naming the field, and the bytes within the field. 所有这些信息都被保存到几个二进制文件里。Lucene为了减少数据的大小，对数据采用了bit packing的压缩方法。举个例子，一个字节可以表示0～127的无符号数，但是如果我们的数据确定最大不会超过63，那就可以只用4位来表示，这样就省去了一半的空间。类似的方法还有ZigZag编码。这部分实现对应代码org.apache.lucene.util.packed。 为了提高搜索效率，Lucene底层查找Term Dictionary(记录了term在文档中的位置信息)时使用了跳跃链表的数据结构。它相对于平衡二叉树简单，但是也有不错的性能，Redies中也有用到。 RefWhat is in a Lucene index Exploring Solr Internals : The Lucene Inverted Index Lucene file format How fast is bit packing 跳跃链表 ZigZag in ProtoBuffers]]></content>
  </entry>
  <entry>
    <title><![CDATA[Valgrind工作原理简介]]></title>
    <url>%2Fhow-valgrind-work%2F</url>
    <content type="text"><![CDATA[Valgrind是一款用于构建内存调试、内存泄露检测以及性能分析的软件开发框架，它是一个由来自全世界的开发者组织合作开发得到的开源软件。他的初始作者Julian Seward在2006年获得第二届Google-O’Reilly开源代码奖。Valgrind这个名字取自北欧神话中英灵殿的入口。 官方首页上这个骑士与恶龙的绘画由伦敦画家 Rupert Lees 创作。灵感来自于神话传说 St George and the Dragon 。某日，圣乔治到利比亚去，当地沼泽中的一只恶龙（一说鳄鱼）在水泉旁边筑巢，这水泉是Silene城唯一的水源，市民为了取水，每天都要把两头绵羊献祭给恶龙。 到后来，绵羊都吃完了，只好用活人来替代，每天抽签决定何人应选派作牺牲。 有一天，国王的女儿被抽中，国王也没有办法，悲痛欲绝。当少女走近，正要被恶龙吞吃时，圣乔治在这时赶到，提起利矛对抗恶龙，并用腰带把它束缚住，牵到城里当众杀死，救出了公主。 在软件开发过程中我们有时需要追踪程序执行、诊断错误、衡量性能等需求。如果有源代码的话可以添加相应代码，没有源代码时这个方法就行不通了。前者可以对源代码分析，后者只能分析二进制了。我的理解是，dynamic Binary Instrumentation (DBI)强调对二进制执行文件的追踪与信息收集，dynamic Binary Analysis (DBA)强调对收集信息的分析，valgrind是一个DBI框架，使用它可以很方便构建一个DBA工具。 Valgrind作者认为当时的DBI框架都把注意力放在了性能检测上，对程序的质量(原文capabilities)并没有太注意。所以设计了一个新的DBI框架，目标是可以使用它开发重型DBA工具。 Valgrind的重点是shadow values技术，该技术要求对所有的寄存器和使用到的内存做shadow（自己维护一份）。因此使用valgrind开发出来的工具一般跑的都比较慢。为了实现shadow values需要框架实现以下4个部分： Shadow State 提供 shadow registers (例如 integer、FP、SIMD) 提供 shadow memory 读写操作 9个具体功能: instrument read/write instructions instrument read/write system calls Allocation and deallocation operations instrument start-up allocations instrument system call (de)allocations instrument stack (de)allocations instrument heap (de)allocations Transparent execution, but with extra output extra output 核心思想就是要把寄存器和内存中的东西自己维护一份，并且在任何情况下都可以安全正确地使用，同时记录程序的所有操作，在不影响程序执行结果前提下，输出有用的信息。使用shadow values技术的DBI框架都使用不同方式实现了上述全部功能或部分功能。 valgrind结构上分为core和tool，不同的tool具有不同的功能。比较特别的是，valgrind tool都包含core的静态链接，虽然有点浪费空间，但可以简化某些事情。当我们在调用valgrind时，实际上启动的只是一个解析命令参数的启动器，由这个启动器启动具体的tool。 为了实现上述功能，valgrind会利用dynamic binary re-compilation把测试程序（client程序）的机器码解析到VEX中间语言。VEX IR是valgrind开发者专门设计给DBI使用的中间语言，是一种RISC like的语言。目前VEX IR从valgrind分离出去成libVEX了。libVEX采用execution-driven的方式用just-in-time技术动态地把机器码转换为IR，如果发生了某些tool感兴趣的事件，就会hook tool的函数，tool会插入一些分析代码，再把这些代码转换为机器码，存储到code cache中，以便再需要的时候执行。 Machine Code --&gt; IR --&gt; IR --&gt; Machine Code ^ ^ ^ | | | translate | | | | instrument | | translate valgrind启动后，core、tool和client都在一个进程中，共用一个地址空间。core首先会初始化必要的组件，然后载入client，建立client的stack，完成后会要求tool初始化自己，tool完成剩余部分的初始化，这样tool就具有了控制权，开始转换client程式。从某种意义上说，valgrind执行的都是加工后的client程序的代码。 DBI framework 有两种基本的方式可以表示code和进行 instrumentation： disassemble-and-resynthesise (D&amp;R)。Valgrind 使用这种把machine code先转成IR，IR会通过加入更IR来instrument。IR最后转回machine code执行，原本的code对guest state的所有影响都必须明确地转成IR，因为最后执行的是纯粹由IR转成的machine code。 copy-and-annotate (C&amp;A)。instructions会被逐字地复制(除了一些 control flow 改变)每个instruction都加上注解描述其影响(annotate)，利用这些描述来帮助做instrumentation通过给每条指令添加一个额外的data structure (DynamoRIO)通过提供相应的获取指令相关信息的API (Intel Pin)这些添加的注解可以指导进行相应的instrument，并且不影响原来的native code的执行效果。 Ref本文主要参考了官方的研究论文（ http://valgrind.org/docs/pubs.html ）。]]></content>
  </entry>
</search>
