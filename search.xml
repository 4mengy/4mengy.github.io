<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Socket program]]></title>
    <url>%2Fsocket-program%2F</url>
    <content type="text"><![CDATA[TCP(Transmission Control Protocol) 是由 IETF 的 RFC 793 定义的一种面向连接的、可靠的、基于字节流的传输层通信协议。 TCP 报文段TCP 数据被封装在一个 IP 数据报中，如下图所示： 下图是 TCP 首部的数据格式，如果不计任选字段，它通常是20个字节： 下面介绍重要的几个数据： 32位序号：表示数据当前发送的第一个字节在字节流中的序号 32位确认号：表示发送端所期望收到的下一个序号，因此该序号位上一次收到的序号加一 6个特殊标志bit: (按照排列顺序) URG: 紧急指针有效 ACK：确认序号有效 PSH：接收方应该尽快将这个报文段交给应用层 RST：重建连接 SYN：同步序号，用来发起一个连接 FIN：发送端完成任务，关闭发送端到接收端连接 其余的解释请参考 TCP/IP 协议详解。 TCP 连接的状态图 TCP 连接的建立与终止TCP 是一个面向连接的通信协议，这要求通信双方在进行通信之前，需要先建立其连接。在常见的客户端、服务器模式的程序中，通常是服务器绑定端口，并在该端口上监听客户端连接请求；客户端主动向服务器发起连接请求，待服务器响应后，双方建立起一条通信链路。 建立TCP 连接建立时通信双方的分组报文如下图所示： 如图所示，客户端调用connect，此时客户端发送SYN报文；服务端调用accept接受该连接请求，同时发送SYN和ACK；等到客户端响应了ACK后，双方建立起完整连接。 将上述过程映射到TCP状态图上进行观察，在服务器端： 刚开始服务器处于CLOSED状态。 服务器初始化时绑定了具体的端口，并调用 listen监听该端口，进入了LISTEN状态。 服务端接收到了来自客户端的SYN报文，发送SYN和ACK给客户端，然后进入SYN_RCVD状态。 当服务端接收到了客户端发送的ACK时，进入ESTABLISHED状态。 客户端方面： 开始同样处于CLOSED状态。 应用主动调用connect发起连接，发送SYN报文给服务器，然后进入 SYN_SEND状态。 当收到服务器的SYN和ACK后，发送对应的ACK给服务器，并进入ESTABLISHED状态。 当双方都进入ESTABLISHED状态时，表示连接已经建立成功。 如果，客户端在发送了SYN报文后，等待超时，重试几次后，便会触发 Timeout进入CLOSED ，在应用层则表示为connect函数调用失败。 关闭连接FIN报文用于通知对方关闭本方向的连接。由于TCP是一个全双工的通信协议，像管道一样，支持关闭某一方向上的连接，所以在TCP中关闭连接需要双方都发送FIN报文。此时分组报文如下图所示： 当某一方关闭连接时，发送FIN报文给另一方，对方回复ACK，同时也发送 FIN报文；等到双方都收到最后的ACK后，连接关闭。当然，如果另一方只回复了ACK而没有发送FIN报文，则表示对方仍然想要发送数据，这种情况称为TCP的半关闭。只有当双方都发送了FIN并接收到对方的ACK后，才算真正的连接关闭。所以上图中Server端的FIN报文可以在接收到Client的FIN报文后，隔一段时间再发送。 在状态图中对应了主动关闭和被动关闭，首先观察主动关闭： 当应用调用close后，发送FIN报文给对方，并由ESTABLISHED状态进入FIN_WAIT_1状态。 收到ACK报文后，进入FIN_WAIT_2状态。 等待对方的FIN报文到达，并发送ACK给对方，进入TIME_WAIT状态。 如果在 FIN_WAIT_1状态直接接收到FIN和ACK，则直接进入TIME_WAIT状态。 TIME_WAIT状态等待了2 MSL后，进入CLOSED状态，此时连接关闭。 被动关闭则简单得多： 当收到对方的FIN报文后，发送ACK并由ESTABLISHED进入 CLOSE_WAIT状态。 用户层调用close后，发送FIN报文同时进入LAST_ACK状态。 接收到对方的 ACK 后，进入CLOSED状态，连接关闭。 TIME_WAIT状态可能时状态图中最不易懂的地方，它也被称为 2 MSL 状态。每一个具体TCP实现必须选择一个报文段最大生存时间MSL(Maximum Segment Lifetime)，表示任何报文段被丢弃前能在网络中存活的时间。当TCP执行主动关闭并发送了ACK给对方进入TIME_WAIT状态后，该连接必须在TIME_WAIT状态停留2倍的MSL。这样可以保证TCP在超时后再次发送最后的ACK报文以防止这个ACK报文丢失。使用2 MSL的另外一点是，当前的socket关闭后，可能立即被用于建立另一个TCP连接，而网络中可能存在着尚未到达具有TIME_WAIT状态一方的包，需要保证这些包不会影响到接下来即将建立的连接。2 MSL的时间间隔中不允许socket被重新使用，同时也能够保证消耗掉网络中的包。所以TIME_WAIT状态存在有两个理由： 可靠地实现 TCP 全双工连接的终止 允许老的重复的包在网络中消逝 关于LAST_ACK状态和TIME_WAITE状态，下面假设：主动关闭的一端为A，被动关闭的一端为B，根据B是否收到最后的ACK包分为两种情况： B发送FIN，进入LAST_ACK状态，A收到这个FIN包后发送ACK包，B收到这个ACK包，然后进入CLOSED状态 B发送FIN，进入LAST_ACK状态，A收到这个FIN包后发送ACK包，由于某种原因，这个ACK包丢失了，B没有收到ACK包，然后B等待ACK包超时，又向A发送了一个FIN包 假如这个时候，A还是处于TIME_WAIT状态(也就是TIME_WAIT持续的时间在2 MSL内)。A收到这个FIN包后向B发送了一个ACK包，B收到这个ACK包进入CLOSED状态 假如这个时候，A已经从TIME_WAIT状态变成了CLOSED状态。A收到这个FIN包后，认为这是一个错误的连接，向B发送一个RST包，当B收到这个RST包，进入CLOSED状态 假如这个时候，A挂了（假如这台机器炸掉了）。B没有收到A的回应，那么会继续发送FIN包，也就是触发了TCP的重传机制，如果A还是没有回应，B还会继续发送FIN包，直到重传超时(至于这个时间是多长需要仔细研究)，B重置这个连接，进入CLOSED状态，参考链接看这里 Ref TCP program TCP/IP State Transition Diagram 在tcp协议中处于last_ack状态的连接，如果一直收不到对方的ack，会一直处于这个状态吗？ Coping with the TCP TIME-WAIT state on busy Linux servers Maximum_segment_lifetime TCP Retransmission: how many packets will be re-sent?]]></content>
  </entry>
  <entry>
    <title><![CDATA[Apache Kylin的Top-N近似预计算]]></title>
    <url>%2FTopN-approximate-precomputation-of-Apache-Kylin%2F</url>
    <content type="text"><![CDATA[在大数据处理场景，由于数据量巨大，某些功能严格准确实现可能会耗费可观的资源，甚至无法计算。这种情况需要牺牲一定的准确性来换取的灵活性和性能。在最近项目中需要定时计算Top N，并提供数天内的Top N，经过考虑后，计划每次计算出Top N * M，然后合并出Top N，当然M越大，最终的结果越准确。但是还是希望找到业界处理类似问题的方案，最终找到了下面这篇文章。 Apache Kylin是一个开源的分布式分析引擎，提供Hadoop之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据。它能在亚秒内查询巨大的数据集 。本文将详细介绍Apache Kylin 1.5中的新功能： Top-N预计算。 大家都听过二八定律，这是在很多领域存在的规律，例如世界上20%的人占有了超过80%的财富；20%最受欢迎的商品，贡献超过了80%的销售额等等。 二八定律背后的规律是Zipf分布法则，它是美国学者G.K.齐普夫在统计英文单词出现频率时发现的规律。简单说就是如果把频率出现最高的单词的频率看作是1的话，第二个出现的频率是二分之一，第三个是三分之一，依此类推，出现的频率是它排名的某次幂分之一。 在互联网时代，还有一个知名的理论－长尾效应，举例来说就是某个网站的用户或者商品的数量非常的多，但是大部分都是访问频率（或价值）极低的，这条尾巴可以很长。长尾的存在对大数据分析带来挑战，因为它的基数（cardinality）特别高，如何从中快速找到高价值的商品或者用户，是一个迫切而难度很高的任务。 现在来看一个典型的Top－N查询示例。该查询是选择在 2015年10月1日，地址在北京，销售商品按价格之和排序（倒序），找前100个。 在Kylin v1.5之前，SQL中的group by列，需声明成维度，所以这个Cube的维度中要有日期，地点和商品名，度量是SUM(PRICE) 。图3展示了一个这样设计Cube。因为商品的基数很大，计算的cuboid的行数会很多；而度量值SUM(PRICE)是非排序的，因此需要将这些纪录都从存储器读到Kylin查询引擎中（内存）， 然后再排序找出最高的纪录；这样的解决办法总开销较大。 针对上面的情形，Kylin开发团队决定另辟蹊径来处理这种查询，研究了多种Top－N的解决方法；由于在大数据的背景下，算法要求一定是可并发执行的，计算结果是需要可再次合并的，而计算结果的少量误差是可以接受的； 最终Kylin选择了Space－Saving算法[1]，以及它的一个衍生版Parallel Space－Saving[2]，并在此之上做了特定的优化。这种算法的优势是使用较少的空间，同时保证较高的精确度。 有了Top－N之后，Cube的设计会比以前简单很多，因为像刚才的商品名会被挪到Measure中去，在Measure里按Sum值做倒序，只保留最大的若干值。 值得一提的是需要用多少空间运算Top－N。简单来说存储空间越多准确率越高。我们通过使用生成一些样本数据然后用Space－Saving计算，并且跟真实结果做比较，发现50倍空间对于普通的数据分布是够用的。也即，用户需要Top 100的结果，Kylin对于每种组合条件值，保留Top 5000的纪录, 并供以后再次合并。这样即使多次合并， Top100依然是比较接近真实结果 。 Top－N的优点：因为它只保留Top的记录，会让Cube空间大幅度减少，而查询性能大大提升。在一个典型的例子里，改用Top-N后，Cube的大小减少了90%，而查询时间则只有以前的10%不到。 缺点是它可能是近似的结果（当50倍空间也无法容纳所有基数的时候）。如果业务场景需要绝对精确的话，它可能不适合。 Top－N误差率由很多因素决定的： 数据的分布：数据分布越陡，误差越小。 算法使用的空间：如果对精度要求高的话，可以选择用更多的空间换取更精准的准确率 。在实际使用中，可以做一些比较以了解误差情况。 [1] Ahmed Metwally, et al. “Efficient computation of frequent and top-k elements in data streams”. Proceeding ICDT’05 Proceedings of the 10th international conference on Database Theory, 2005. [2] Massimo Cafaro, et al. “A parallel space saving algorithm for frequent items and the Hurwitz zeta distribution”. Proceeding arXiv: 1401.0702v12 [cs.DS] 19 Setp 2015. Ref Apache Kylin的Top-N近似预计算]]></content>
  </entry>
  <entry>
    <title><![CDATA[Why numbering should start at zero]]></title>
    <url>%2FWhy-numbering-should-start-at-zero%2F</url>
    <content type="text"><![CDATA[To denote the subsequence of natural numbers 2, 3, …, 12 without the pernicious three dots, four conventions are open to us a) 2 ≤ i &lt; 13 b) 1 &lt; i ≤ 12 c) 2 ≤ i ≤ 12 d) 1 &lt; i &lt; 13 Are there reasons to prefer one convention to the other? Yes, there are. The observation that conventions a) and b) have the advantage that the difference between the bounds as mentioned equals the length of the subsequence is valid. So is the observation that, as a consequence, in either convention two subsequences are adjacent means that the upper bound of the one equals the lower bound of the other. Valid as these observations are, they don’t enable us to choose between a) and b); so let us start afresh. There is a smallest natural number. Exclusion of the lower bound —as in b) and d)— forces for a subsequence starting at the smallest natural number the lower bound as mentioned into the realm of the unnatural numbers. That is ugly, so for the lower bound we prefer the ≤ as in a) and c). Consider now the subsequences starting at the smallest natural number: inclusion of the upper bound would then force the latter to be unnatural by the time the sequence has shrunk to the empty one. That is ugly, so for the upper bound we prefer &lt; as in a) and d). We conclude that convention a) is to be preferred. Remark The programming language Mesa, developed at Xerox PARC, has special notations for intervals of integers in all four conventions. Extensive experience with Mesa has shown that the use of the other three conventions has been a constant source of clumsiness and mistakes, and on account of that experience Mesa programmers are now strongly advised not to use the latter three available features. I mention this experimental evidence —for what it is worth— because some people feel uncomfortable with conclusions that have not been confirmed in practice. (End of Remark.) * * * When dealing with a sequence of length N, the elements of which we wish to distinguish by subscript, the next vexing question is what subscript value to assign to its starting element. Adhering to convention a) yields, when starting with subscript 1, the subscript range 1 ≤ i &lt; N+1; starting with 0, however, gives the nicer range 0 ≤ i &lt; N. So let us let our ordinals start at zero: an element’s ordinal (subscript) equals the number of elements preceding it in the sequence. And the moral of the story is that we had better regard —after all those centuries!— zero as a most natural number. Remark Many programming languages have been designed without due attention to this detail. In FORTRAN subscripts always start at 1; in ALGOL 60 and in PASCAL, convention c) has been adopted; the more recent SASL has fallen back on the FORTRAN convention: a sequence in SASL is at the same time a function on the positive integers. Pity! (End of Remark.) * * * The above has been triggered by a recent incident, when, in an emotional outburst, one of my mathematical colleagues at the University —not a computing scientist— accused a number of younger computing scientists of “pedantry” because —as they do by habit— they started numbering at zero. He took consciously adopting the most sensible convention as a provocation. (Also the “End of …” convention is viewed of as provocative; but the convention is useful: I know of a student who almost failed at an examination by the tacit assumption that the questions ended at the bottom of the first page.) I think Antony Jay is right when he states: “In corporate religions as in others, the heretic must be cast out not because of the probability that he is wrong but because of the possibility that he is right.” Ref EWD831]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker:pid1 zombie and signal problem]]></title>
    <url>%2Fdocker-pid1-zombie-and-signal-problem%2F</url>
    <content type="text"><![CDATA[在类Unix系统中，pid是1的进程是一个很特殊的进程。类Unix操作系统的进程管理采用树形结构，用户空间中的其他进程都直接或间接是pid 1进程的子进程。在Mac OSX中该进程是launchd，Linux中有多种选择，例如systemd，一般将其称作init进程。如果init进程意外退出，内核会panic。为了避免意外操作，pid为1的进程不会使用系统默认的信号处理函数，要处理信号，必须显式指定信号处理函数。 init进程还承担着一些其他职责。由于所有用户空间的进程都是其子进程，所以，当进程结束时，其需要处理进程的资源回收。另外，某些情况下，父进程可能先于子进程退出，此时子进程没有了父进程变成所谓的‘孤儿’进程，init负责将此类进程收养为自己的子进程，并负责后续的资源回收。 Linux pid namespace技术可以实现pid资源的隔离，广泛应用于各种容器技术中，例如docker。但是在docker的早期版本中对init进程处理不当引起了一些问题，尤其是当多进程程序运行在容器中时。 容器启动时，运行的程序默认会被分配为pid 1，但是一般情况下，我们都没有假设我们的程序会以pid 1来运行。 这里可能面临僵尸进程的问题。看下面的例子，该程序fork了两次，一共创建了3个进程：父亲、儿子和孙子进程。儿子进程先结束，由于父进程没有任何处理，儿子进程变成僵尸进程，孙子进程变成孤儿进程，等待被收养； 1234567891011121314151617181920212223242526272829#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;void main()&#123; pid_t pid = fork(); if (pid == 0) &#123; printf("[C] child process.\n"); //avoid buffer get into child process fflush(stdout); pid_t pid2 = fork(); if (pid2 == 0) &#123; printf("[C] 2nd fork, child process.\n"); sleep(15); &#125; else &#123; printf("[P] 2nd fork, child process pid: %d.\n", pid2); sleep(8); &#125; &#125; else &#123; printf("[P] child process pid: %d.\n", pid); sleep(20); &#125;&#125; 以上代码进行编译后，生成的可执行文件位于/tmp/test/a.out。将该目录映射到容器的/root/目录。 docker run -itd –name foo –rm -v /tmp/test/:/root/ ubuntu /root/a.out 以上命令执行后，在另一个窗口执行 docker exec -it foo /bin/bash 这样会打开一个容器的bash界面，通过它可以观察容器内程序的执行情况。一开始三个进程都在执行。 PPID PID PGID SID TTY TPGID STAT UID TIME COMMAND 0 8 8 8 pts/1 15 Ss 0 0:00 /bin/bash 8 15 15 8 pts/1 15 R+ 0 0:00 \_ ps -jxf 0 1 1 1 pts/0 1 Ss+ 0 0:00 /root/a.out 1 6 1 1 pts/0 1 S+ 0 0:00 /root/a.out 6 7 1 1 pts/0 1 S+ 0 0:00 \_ /root/a.out pid 6儿子进程结束，僵尸状态，pid 7孙子进程被init进程收养，即pid 1进程。 PPID PID PGID SID TTY TPGID STAT UID TIME COMMAND 0 8 8 8 pts/1 22 Ss 0 0:00 /bin/bash 8 22 22 8 pts/1 22 R+ 0 0:00 \_ ps -jxf 0 1 1 1 pts/0 1 Ss+ 0 0:00 /root/a.out 1 6 1 1 pts/0 1 Z+ 0 0:00 [a.out] 1 7 1 1 pts/0 1 S+ 0 0:00 /root/a.out pid 7孙子进程结束，由于pid 1进程没有对其回收资源，故也进入僵尸状态。 PPID PID PGID SID TTY TPGID STAT UID TIME COMMAND 0 8 8 8 pts/1 23 Ss 0 0:00 /bin/bash 8 23 23 8 pts/1 23 R+ 0 0:00 \_ ps -jxf 0 1 1 1 pts/0 1 Ss+ 0 0:00 /root/a.out 1 6 1 1 pts/0 1 Z+ 0 0:00 [a.out] 1 7 1 1 pts/0 1 Z+ 0 0:00 [a.out] 最后当父进程结束后，整个容器结束，所有资源被清理。 解决以上问题，需要容器内部运行一个有效的init进程，使用systemd这种高级程序会使容器臃肿，可以使用一些很轻便的实现，甚至自己写一个满足自己需求的程序。 现有的解决方案： 使用baseimage-docker 使用docker内置的init工具tini，在docker run命令加上参数–init。 Ref Docker and the pid 1zombie reaping problem Docker - init, zombies - why does it matter? The Curious Case of Pid Namespaces DOCKER DEMONS: PID-1, ORPHANS, ZOMBIES, AND SIGNALS Cannot kill or detach a docker container using Ctrl-C or Ctrl-\ My process became PID 1 and now signals behave strangely]]></content>
  </entry>
  <entry>
    <title><![CDATA[how to write resilient mongodb applications]]></title>
    <url>%2Fhow-to-write-resilient-mongodb-applications%2F</url>
    <content type="text"><![CDATA[前言：本文翻译自MongoDB团队工程师A. Jesse Jiryu Davis的博客文章。文章描述了在遇到错误时如何安全的进行重试。虽然这项技术已被MongoDB 3.6的新内置功能Retryable Writes取代。 可重写的写操作比这里描述的技术简单得多。但是作者解决问题的方式还是能给我们一些启发。 有一次，在2012年初的一个寒冬午后，我遇到了一位非常生气的MongoDB客户。 他来到我们的“MongoDB Office Hours”办公室，他有一个问题：“我怎样才能使我的应用程序在网络错误，中断和其他异常情况下具有弹性？我可以每次重试操作直到成功？“他要求知道为什么我们没有发布一个适用于所有应用程序的简单明智的策略。 这个人，我会叫他Ian，很不高兴，我也帮不了他。我的内疚已经让我忘记了那天的细节。我们坐在一间没有窗户的房间里，这是我们唯一可以在我们的小办公室聊天的自由空间。这里有一个讨厌的荧光灯。我们并排坐在桌子的边缘，因为房间没有椅子。在Ian的眼睛下有深深的黑眼圈，就像他已经为这个问题担心了一整晚。 “我如何编写弹性代码？” 我唯一可以告诉Ian的是，“我们无法发布一个策略来处理网络错误和中断，并为您指出错误，因为我们不知道您应用程序的细节。您可以选择很多不同的操作方式，在延迟和可靠性之间做出折中，以及在一个操作肯能被执行两次的风险与完全不做这两者之间进行权衡，这就是为什么我们没有尝试写出一种“一刀切”的策略。就算可以，但是不同语言下驱动的行为不同，我们必须为每种语言发布指南。“ 我对我的回答并不满意，Ian也一样。 在此后的几年中，我努力想出了一个更好的答案。首先我写了服务发现和监测规范，我们所有的驱动程序现在已经实现了。该规范极大地提高了驱动程序在面对网络错误和服务器故障时的鲁棒性。像Ian这样的投诉现在比较少见，因为驱动很少会抛出异常。此外，所有驱动程序现在都表现相同，我们使用标准化的通用测试套件验证其行为。 其次，我开发了一种名为‘Black Pipe’的测试技术，因此Ian可以测试他的代码在与MongoDB交互时如何响应网络故障，命令错误或任何其他事件。‘Black Pipe’使用方便且准确;它使得错误情况可重现且易于测试。 现在可以回答Ian了。如果他今天来到我们在时代广场的大办公室，你会如何回答他？编写弹性MongoDB应用程序的策略是什么？ 我们会告诉Ian，如何做到这一点： 123updateOne(&#123;'_id': '2016-06-28'&#125;, &#123;'$inc': &#123;'counter': 1&#125;&#125;, upsert=True) 该操作通过在_id为今天日期的文档中递增名为“counter”的字段来计数事件发生的次数。如果这是今天的第一个事件，参数“upsert = True”告诉MongoDB创建文档。 什么会出错瞬态错误当Ian将他的updateOne消息发送到MongoDB时，驱动程序可能会看到来自网络层的暂时错误，例如TCP重置或超时。 瞬态网络错误，故障转移或降级驱动不知道服务器是否收到消息，所以Ian不知道他的计数器是否增加。 还有其他瞬态错误看起来与网络暂时性问题相同。如果primary节点出现故障，下次尝试向其发送消息时，驱动程序会收到网络错误。这个错误很简短，因为副本集会在几秒钟内选出一个新的primary节点。同样，如果primary服务器退出（它仍然正常工作，但已经辞去其primary角色），它将关闭所有连接。下次驱动程序向服务器发送消息时，它认为它是primary节点，它会从服务器收到网络错误或“not master”回复。 在所有情况下，驱动程序都会向Ian的应用程序抛出连接错误。 持续的错误也可能存在持续的网络中断。当驱动程序首次检测到此问题时，它看起来像一个暂时性问题：驱动程序发送一条消息，但是无法读取响应。 Persistent Network Outage 持续的网络中断Ian再一次无法分辨服务器是否接收到消息并递增计数器。 网路中断与网络暂时性问题的区别是Ian试图再次操作只会得到另一个网络错误。但是只有他尝试后才知道。 命令错误当驱动程序发送消息时，MongoDB可能会返回一个特定的错误，表示已收到命令但无法执行。也许命令格式不正确，服务器磁盘空间不足，或Ian的应用程序未被授权。 所以，有三个不能完全可区分的错误，需要Ian的代码有不同回应。你会给Ian什么样的策略来使他的应用具有弹性？ 是因为我们没有事务吗？您可能想知道这是否是只有MongoDB才有的问题，因为它没有事务。假设Ian使用了传统的SQL服务器。他打开一个事务，更新一行，并发送COMMIT消息。然后有一个网络临时性错误。他无法从服务器读取确认消息。他知道交易是否已经实施吗？ 保证操作只执行一次，使用SQL服务器与使用非事务性服务器（如MongoDB）面临的问题相同。 MongoDB驱动程序如何处理错误？为了制定您的策略，您需要知道MongoDB驱动程序自己如何响应不同类型的错误。 服务器发现和监测规范要求MongoDB驱动程序跟踪每个连接到的服务器的状态。例如，它可能是一个3节点副本集，如下所示： Server 1: Primary Server 2: Secondary Server 3: Secondary 这个数据结构被称为“拓扑描述”。如果在与服务器交谈时出现网络错误，驱动程序会将该服务器的类型设置为“未知”，然后引发异常。现在拓扑描述是： Server 1: Unknown Server 2: Secondary Server 3: Secondary Ian尝试的操作不会自动重试; 然而，下一个操作会被阻塞，因为驱动程序正在重新发现主驱动器。它每秒重新检查每台服务器两次，最多持续30秒，直到重新连接到主服务器或检测到新主服务器被选中。现在MongoDB的选举只需要一两秒钟，然后驱动程序在此之后大约半秒会收到选举结果。 另一方面，如果网络持续中断，则在30秒之后，驱动程序会抛出“server selection timeout”。 在命令错误的情况下，驱动程序对服务器的看法没有改变：如果服务器是primary或secondary，它仍然是。因此，驱动程序不会在拓扑描述中更改服务器的状态，只会引发异常。 （要了解有关服务器发现和监测规范的更多信息，请阅读我的文章PyMongo，Perl和C中的服务器发现和监控，或观看MongoDB World 2015上的演讲，MongoDB驱动程序和高可用性：深度分析。在那里我详细讨论了规范中描述的数据结构以及规范如何告诉驱动程序对错误做出反应，这是本文关于弹性应用程序的一个很好的背景。） 错误的重试策略我见过一些。 不要重试默认是根本不重试。这种策略在遇到前面描述的三种错误情况时可能会失败。 瞬态网络错误 持续中断 命令错误 可能计数丢失 正确 正确 在发生瞬时网络错误的情况下，Ian将消息发送到服务器，并且不知道服务器是否收到它。如果没有，该事件从不计算在内。他的代码可能会记录错误，然后继续前进。 有趣的是，面对长期的网络中断或命令错误，“不要重试”是正确的策略，因为这些是非瞬态错误，无法通过重试改善。 总是重试一些程序员编写代码重试任何失败的操作五次，或者，如果他们真的关心弹性，十次。我在很多生产应用程序中都看到了这一点。 123456789i = 0while True: try: do_operation() break except network error: i += 1 if i == MAX_RETRY_COUNT: throw 去年，我与一位名为Sam的Rackspace工程师进行了交谈。他接手了一个应用了这个不好的策略的Python代码库：它只要接收到任何异常就会一遍又一遍地重试。 Sam认为这是愚蠢的。他注意到我最近根据服务器发现和监测规范重新编写了PyMongo的客户端代码，他推测他可以更好地利用更强大的新驱动程序。在那里有一个更聪明的重试策略，但他不知道它到底是什么。事实上，正是从我与Sam的对话中，这篇文章诞生了。 Sam看到了什么？为什么Ian不应该重试每次操作五到十次？ 瞬态网络错误 持续中断 命令错误 可能过多计数 浪费时间 浪费时间 在网络出现瞬态错误的情况下，Ian不会丢失计数，现在他可能会计数过多。因为如果服务器在发生网络错误之前先读取其第一条updateOne消息，则第二条updateOne消息会再次递增计数器。 另一方面，在持续的网络中断期间，多次重试是浪费时间。第一次网络错误后，驱动程序将primary服务器标记为“unknown”; 当Ian重试该操作时，它会在驱动程序尝试重新连接时阻塞，每秒检查两次，持续30秒。如果驱动程序代码中的所有努力都没有成功，那么再次尝试进入驱动程序的重试循环是徒劳的。这会导致他的应用程序出现排队和延迟。 命令错误也是如此：如果Ian的应用程序未被授权，则重试五次不会改变该情况。 重试一次网络错误现在我们比较接近一个明智的策略。在Ian初始操作失败并出现网络错误后，Ian不知道错误是暂时的还是持久的，因此他只重试一次操作。单次重试进入驱动程序的30秒重试循环。如果网络错误持续30秒，则可能会持续更长时间，所以Ian放弃了继续重试。 但是，面对命令错误，他根本不会重试。 瞬态网络错误 持续中断 命令错误 可能过多计数 正确 正确 所以我们只剩下一种情况还有问题 - Ian怎么避免过多计数的可能性？ 重试网络错误，使操作幂等幂等性操作无论是一次还是多次执行都具有相同的结果。如果Ian的所有操作都是幂等的，那么他可以安全地重试它们，而不会发生过多计数或任何由于发送消息两次带来的其他类型错误。 瞬态网络错误 持续中断 命令错误 正确 正确 正确 那么Ian应该如何让他的updateOne操作幂等？ 幂等操作MongoDB有四种操作：查找，插入，删除和更新。前三个很容易幂等性; 让我们先来处理它们。 Find查询自然是幂等的。 1234try: doc = findOne()except network err: doc = findOne() 两次检索文档与检索一次文档结果一样。 Insert比查询困难一点，但不是太糟糕。 123456789doc = &#123;_id: ObjectId(), ...&#125;try: insertOne(doc)except network err: try: insertOne(doc) except DuplicateKeyError: pass # first try worked throw 这个伪代码的第一步是在客户端生成一个唯一的ID。 MongoDB的ObjectId是为这种用法而设计的，但任何独特的值都可以。 现在，Ian试图插入文档。如果因网络错误而失败，他会再次尝试。如果第二次尝试失败且服务器出现重复键错误，则第一次尝试成功，但由于网络层发生错误，他无法读取服务器响应。如果出现其他问题，他取消操作并抛出异常。 这个插入是幂等的。唯一需要警告的是，我假设Ian除了MongoDB自动创建的_id之外，没有唯一索引。如果还有其他唯一索引，那么他必须解析重复键错误。 Delete如果Ian删除一个文档时指定一个唯一的key，那么执行两次与执行一次相同。 1234try: deleteOne(&#123;'key': uniqueValue&#125;)except network err: deleteOne(&#123;'key': uniqueValue&#125;) 如果第一个被执行，但是Ian得到一个网络异常并再次尝试，那么第二个删除就是空操作;它只是不匹配任何文档。该删除可以安全地重试。 同时删除许多文件甚至更容易： 1234try: deleteMany(&#123;...&#125;)except network err: deleteMany(&#123;...&#125;) 如果Ian删除所有匹配过滤器的文档，并且他的代码出现网络错误，那么他可以再次安全地尝试。无论是deleteOne运行一次还是两次，结果都是一样的：所有匹配的文档都被删除。 在这两种情况下都存在竞争条件：另一个进程可能会在两次尝试删除它们之间插入新的匹配文档。但是这场比赛并不比他的代码没有重试更糟糕。 Update那么更新呢？首先考虑那种自然而然是幂等的更新，让我们放松一下。 12345# Idempotent update.updateOne(&#123; '_id': '2016-06-28'&#125;, &#123;'$set':&#123;'sunny': True&#125;&#125;, upsert=True) 这与我们原来的例子不一样; Ian并没有增加一个变量的值，他把今天的“sunny”字段设置为真，让自己振作起来。说两次今天是阳光明媚的，就像说一次一样好。在这种情况下，updateOne可以安全地重试。 一般原则是有一些MongoDB更新运算符是幂等的，有些则不是。 $set是幂等的，因为如果两次将某些值设置为相同的值，则不会有任何区别，而$inc不是幂等的。 如果Ian的更新操作符是幂等的，那么他可以很容易地重试它：他尝试一次，如果他得到网络异常，他会再次尝试。 123456try: updateOne(&#123;' _id': '2016-06-28'&#125;, &#123;'$set':&#123;'sunny': True&#125;&#125;, upsert=True)except network err: try again, if that fails throw 所以现在我们终于准备好解决我们最难的例子，原始的非幂等updateOne： 123updateOne(&#123; '_id': '2016-06-28'&#125;, &#123;'$inc': &#123;'counter': 1&#125;&#125;, upsert=True) 如果Ian偶然做了两次这样的操作，他会将计数增加2。 我们如何将它变成幂等操作？我们将分成两步。每一步都是幂等的，通过将它转换成一对幂等运算，我们将安全地重试。 我们假设一开始文档的counter值是N： 1234&#123; _id: '2016-06-28', counter: N&#125; 在第一步中，Ian先不管counter当前的值，他只是给一个“pending”数组添加一个标记。这里他需要一些独特的东西; 一个ObjectId是很好的选择： 1234567oid = ObjectId()try: updateOne(&#123; '_id': '2016-06-28'&#125;, &#123;'$addToSet': &#123;'pending': oid&#125;&#125;, upsert=True)except network err: try again, then throw $addToSet是幂等运算符之一。如果它运行两次，令牌只会添加一次到数组中。现在文档看起来像这样： 12345&#123; _id: '2016-06-28', counter: N, pending: [ ObjectId("...") ]&#125; 第二步，对于单个操作，Ian通过_id及其待处理标记查询文档，删除待处理标记并增加计数器。 123456789try: # Search for the document by _id and pending token. updateOne(&#123;'_id': '2016-06-28', 'pending': oid&#125;, &#123;'$pull': &#123;'pending': oid&#125;, '$inc': &#123;'counter': 1&#125;&#125;, upsert=False)except network err: try again, then throw 所有的MongoDB更新，无论它们是否是幂等的，都是原子的：单个updateOne完全成功或根本没有任何作用。所以只有当计数器增加1时, 令牌才会从待处理数组中移除。 总的来说，这个更新是幂等的。想象一下，Ian将令牌移出数组并在第一次尝试时递增计数器，但是之后他无法读取服务器响应，因为存在网络错误。他第二次尝试是无效的，因为查询要求匹配具有待处理令牌的文档，但他已经将其取出。 所以Ian可以安全地重试这​​个updateOne。无论它执行了一次还是两次，文档结果都是一样的： 12345&#123; _id: '2016-06-28', counter: N + 1, pending: [ ]&#125; 任务完成？现在你已经做到了：你重现实现了Ian原来的updateOne，这是不安全的重试，并通过将它分成两步来使它变得幂等。 这项技术有几点需要注意。其中之一是Ian简单的$inc操作现在需要两次往返。它需要延迟两倍，负载加倍。如果在网络瞬态错误中被少计入一次或超过一次没有什么问题，他就不应该使用这种技术。 另一个警告是，他需要一个每晚执行的清理过程。考虑一下：如果第二步从未完成，会发生什么？ 12345678try: updateOne(&#123;'_id': '2016-06-28', 'pending': oid&#125;, &#123;'$pull': &#123;'pending': oid&#125;, '$inc': &#123;'counter': 1&#125;&#125;, upsert=False)except network err: try again, then throw 假设Ian在添加待处理令牌后，将其移除并增加计数器之前，网络中断开始。该文件处于这种状态： 12345&#123; _id: '2016-06-28', counter: N, pending: [ ObjectId("...") ]&#125; Ian需要一个每晚执行的任务来查找今天中止的任务遗留的任何待处理令牌，并完成更新计数器。为了避免并发问题，他一直等到一天结束。他的任务使用聚合管道来查找具有待处理令牌的文档，并将其数量添加到当前计数中： 1234567891011121314151617181920pipeline = [&#123; '$match': &#123;'pending.0': &#123;'$exists': True&#125;&#125;&#125;, &#123; '$project': &#123; 'counter': &#123; '$add': [ '$counter', &#123;'$size': '$pending'&#125; ] &#125; &#125;&#125;]for doc in collection.aggregate(pipeline): collection.updateOne( &#123; '_id': doc._id&#125;, &#123; '$set': &#123;'counter': doc.counter&#125;, '$unset': &#123;'pending': True&#125; &#125;) 对于每个聚合操作结果，此任务使用最终计数器值更新源文档，并清除待处理数组。 updateOne可以安全地重试，因为它使用幂等运算符$set和$unset。Ian的清理任务可以一直尝试，直到它成功，无论他的网络有多么糟糕。执行此清理任务后，Ian今天的事件数量最终将是正确的。 如果Ian接受这些缺点 - 一次增加需要两次往返，并且可能在一天结束之前不能完成 - 那么对于非常关键的操作，这种技术是一种可靠的方式来使其计数器只增加一次。 Ref How To Write Resilient MongoDB Applications Retryable Writes]]></content>
  </entry>
  <entry>
    <title><![CDATA[Deadlock caused by forking a multithreaded process]]></title>
    <url>%2Fdeadlock-caused-by-forking-a-multithreaded-process%2F</url>
    <content type="text"><![CDATA[最近生产环境的一个模块突然卡住了，模块的作者已经去其他组了，没办法只能自己查问题了🤣。 该模块使用python的logging标准库记录日志，自定义kafka作为handler，日志通过kafka发送到Elasticsearch，然后就是ELK日志处理了。 模块为了提高吞吐，内部使用了多进程，问题恰恰就出现在了这里。 使用GDB attach到卡死的进程，执行py-bt命令，输出如下： Traceback (most recent call first): File “/usr/lib64/python2.7/multiprocessing/forking.py”, line 135, in poll pid, sts = os.waitpid(self.pid, flag) File “/usr/lib64/python2.7/multiprocessing/forking.py”, line 154, in wait return self.poll(0) File “/usr/lib64/python2.7/multiprocessing/process.py”, line 145, in join res = self._popen.wait(timeout) File “parser/parser_main.py”, line 76, in data_parse process.join() 主进程里使用multiprocessing模块创建多进程执行任务，这里主进程在等待子进程执行结束。 阅读multiprocessing模块的源代码，可以发现其创建子进程时调用了os.fork()函数，该函数是python os模块对系统调用fork的封装。 子进程的输出： Traceback (most recent call first): Waiting for the GIL File “sdk/python-lib/kafka/producer/record_accumulator.py”, line 223, in append with self._tp_locks[tp]: File “sdk/python-lib/kafka/producer/kafka.py”, line 516, in send self.config[‘max_block_ms’]) File “sdk/python-lib/elk_logging/handlers/kafka.py”, line 37, in emit self.format(record).encode(‘utf-8’)) elk_logging模块自定义了kafka handler，模块导入时通过logging.config.dictConfig()配置logging模块。kafak发送消息时获取不到锁self._tp_lock[tp]，然后一直等待。 在Google上搜索“kafka self._tp_lock deadlock”没有发现有价值的信息，基本排除是kafka库的bug，如果是kafka库的bug，网上不太可能一点线索也没有。 前文我们提到这里使用了fork创建子进程，当内核执行fork时，生成的子进程是父进程的副本，子进程获得父进程的数据空间、堆和栈的副本（这里是子进程自己拥有的副本，并不与父进程共享这些存储空间。很多实现使用了写时复制技术，一开始这些区域被父子进程共享，如果任意一个进程修改这些区域，则内核只为修改的那块内存制作一个副本，通常是虚拟内存的一页）。 在Linux系统中调用fork，创建的子进程中只有一个线程——父进程中调用fork函数的线程。 Note the following further points: The child process is created with a single thread—the one that called fork(). The entire virtual address space of the parent is replicated in the child, including the states of mutexes, condition variables, and other pthreads objects; the use of pthread_atfork(3) may be helpful for dealing with problems that this can cause. 也不是所有的系统都是这样，Solaris系统libthread库提供的fork函数会“复制“所有线程。 Solaris libthread supports both fork() and fork1(). The fork() call has “fork-all” semantics–it duplicates everything in the process, including threads and LWPs, creating a true clone of the parent. The fork1() call creates a clone that has only one thread; the process state and address space are duplicated, but only the calling thread is cloned. POSIX libpthread supports only fork(), which has the same semantics as fork1() in Solaris threads. POSIX标准定义的行为是fork只“复制”一个线程。 那么这些与上面的死锁有什么关系？python标准库logging模块在创建logger时会调用到以下代码，可以看到一个logger只会实例化一次，对应的handler和formatter等也只会实例化一次。 logging/__init__.py manager.getLogger(name)123456789101112131415161718_acquireLock()try: if name in self.loggerDict: rv = self.loggerDict[name] if isinstance(rv, PlaceHolder): ph = rv rv = (self.loggerClass or _loggerClass)(name) rv.manager = self self.loggerDict[name] = rv self._fixupChildren(ph, rv) self._fixupParents(rv) else: rv = (self.loggerClass or _loggerClass)(name) rv.manager = self self.loggerDict[name] = rv self._fixupParents(rv)finally: _releaseLock() 如果自定义了handler，配置logging模块时会导入指定的handler class，并实例化。 logging/config.py DictConfigurator.configure_handler()123456789101112131415161718if '()' in config: c = config.pop('()') if not callable(c): c = self.resolve(c) factory = celse: cname = config.pop('class') klass = self.resolve(cname) factory = klassprops = config.pop('.', None)kwargs = dict([(k, config[k]) for k in config if valid_ident(k)])try: result = factory(**kwargs)except TypeError as te: if "'stream'" not in str(te): raise kwargs['strm'] = kwargs.pop('stream') result = factory(**kwargs) 父进程和fork出来的子进程使用的时相同的对象实例，相同的logger，相同的kafka handler。 kafka producer发送消息是多线程异步的。KafkaProducer实例化时会启动一个消息发送线程。有消息过来后，先通过RecordAccumulator类实例将消息追加到内部list，然后唤醒self._sender发送线程，发送线程通过RecordAccumulator类实例检查哪些可以发送，整个过程RecordAccumulator类实例维护了topic+partition相关的锁。1234567891011121314def __init__(self, **configs): ... self._accumulator = RecordAccumulator(message_version=message_version, metrics=self._metrics, **self.config) self._sender = Sender(client, self._metadata, self._accumulator, self._metrics, guarantee_message_order=guarantee_message_order, **self.config) self._sender.daemon = True self._sender.start() self._closed = False self._cleanup = self._cleanup_factory() atexit.register(self._cleanup) log.debug("Kafka producer started") 如果fork时RecordAccumulator类实例中某个topic+partition的锁还没有释放，子进程中该锁永远不会被释放，因为没有了发送线程，如果子进程中恰好向同一个topic和同一个partition发送消息就会死锁。 由于子进程不会常驻在后台，所以满足上述条件还是比较难的，从表现上来看，5个月里发生了一次死锁。 后记避免这种问题，最好的办法就是不要fork一个多线程进程。 python的标准库logging模块曾经因为类似的原因出过问题。现在官方文档已经指出： Note that safely forking a multithreaded process is problematic. Google甚至还创建过一个项目python-atfork，模仿pthread_atfork，可以注册函数，保正多线程的情况下可以安全的fork。当然，该项目已经被废弃了。 如果使用较新的python版本（3.5+ ？）可以指定multiprocessing模块创建进程的方式，可以尝试spawn方式。 Ref UNIX环境高级编程 Linux fork manual Special Issues for fork() and Solaris Threads python multiprocessing doc Safe to call multiprocessing from a thread in Python？ reentrancy, thread safe, asynchronous signal safe, interrupt safe and cancellation safe Threads and fork(): think twice before mixing them]]></content>
  </entry>
  <entry>
    <title><![CDATA[Debug Python with GDB in off-line CentOS]]></title>
    <url>%2Fdebug-python-with-gdb-in-offline-centos%2F</url>
    <content type="text"><![CDATA[GDB不仅可以调试C/C++程序，它还可以调试Python等程序，是非常重要和有力的开发工具之一。大部分情况下，都可以直接安装发行版提供的gdb来调试程序，但是，某些情况下需要自己编译，例如，机器无法上网；甚至没有权限修改系统，无法安装软件。下面以CentOS为例，介绍如何在离线并且没有全部权限的情况下使用GDB调试Python程序。 首先，需要在相同系统版本可以上网的机器上编译GDB wget -c http://ftp.gnu.org/gnu/gdb/gdb-8.0.1.tar.gz tar xvf gdb-8.0.1.tar.gz cd gdb-8.0.1.tar.gz mkdir build cd build ../configure --enable-static --with-python --disable-interprocess-agent --with-lzma make 需要拿到离线机器使用，所以这里使用‘–enable-static –disable-interprocess-agent’，期望获得一个静态链接的二进制文件。通过‘–with-python’启用了python调试功能。‘–with-lzma’启用了lzma支持，由于某些发行版分发的二进制文件使用了‘MiniDebugInfo’特性：在二进制文件中存在一个名为‘gnu_debugdata’的特殊区域，该区域保存了经过lzma压缩的额外符号信息。 如果在configure过程中报错，一般都是缺少一些依赖包，根据提示进行安装就可以了。 编译完成后，需要的文件都在build/gdb目录内，将他们复制到另一个目录 mkdir -p ~/gdb/data-directory cd gdb cp gdb ~/gdb/gdb cp -r data-directory/* ~/gdb/data-directory cd ~/gdb chmod +x ./gdb data-directory目录内保存了gdb需要使用的数据文件。现在就可以使用这个gdb在离线系统上简单调试程序了 ./gdb --data-directory=./data-directory your-program 但是当进入到一些库函数时，会出现错误，因为系统上没有相应的调试信息文件。需要手动部署这些文件，以调试Python为例，首先获取系统中python包的版本 yum list installed | grep ^python.x86_64 假如得到的信息是‘python.x86_64 2.7.5-58.el7’，去debuginfo.centos.org下载相应的调试信息文件，例如，’python-debuginfo-2.7.5-58.el7.x86_64.rpm‘。 将该文件解包 rpm2cpio python-debuginfo-2.7.5-58.el7.x86_64.rpm | cpio -idmv 正常情况下，该文件是通过rpm安装到系统里的，安装后的结构和直接解压出来的结构可能不同，系统目录： ls -l /usr/lib/debug/ drwxr-xr-x. 3 root root 64 11月 5 2016 . dr-xr-xr-x. 44 root root 4096 10月 28 21:38 .. lrwxrwxrwx. 1 root root 7 10月 28 21:21 bin -&gt; usr/bin lrwxrwxrwx. 1 root root 7 10月 28 21:21 lib -&gt; usr/lib lrwxrwxrwx. 1 root root 9 10月 28 21:21 lib64 -&gt; usr/lib64 lrwxrwxrwx. 1 root root 8 10月 28 21:21 sbin -&gt; usr/sbin drwxr-xr-x. 6 root root 65 10月 28 21:21 usr 解包目录： ls -al ./usr/lib/debug/ drwxr-xr-x. 5 buildbot buildbot 46 10月 29 23:26 . drwxrwxr-x. 3 buildbot buildbot 19 10月 29 23:26 .. drwxr-xr-x. 114 buildbot buildbot 4096 10月 29 23:26 .build-id drwxr-xr-x. 2 buildbot buildbot 40 10月 29 23:26 .dwz drwxr-xr-x. 4 buildbot buildbot 30 10月 29 23:26 usr 系统目录中的符号链接，解包目录里是没有的，需要手动加上。解包目录里的‘.build-id’文件夹是非常重要的文件夹。 GDB允许将调试信息和可执行代码分开存放，因为一般情况，调试信息比可执行代码大很多，普通用户不需要调试信息。分开存放很好的解决了此问题。 GDB支持两种指定调试信息文件的方法： 可执行文件包含一个‘debug link’，其指定了调试文件的名字。通常调试信息文件的名字是‘executable.debug’，其中，‘executable’是可执行文件的名字（不包括路径）。并且，‘debug link’还包含了可执行文件的CRC32值，GDB可以利用该值判断可执行文件和调试信息文件是否来自相同的build。 可执行文件包含一个特殊的字符串‘build-id’，该字符串同时也存在于调试信息文件中。调试信息文件的名字可以通过‘build-id’计算出来。 对于以上两种不同的方式，GDB采用两种方法查找调试信息文件： 对于‘debug link’方式，GDB先在可执行文件所在目录搜索指定名字的调试信息文件，然后是可执行文件所在目录中名为‘.debug’的子目录里搜索，最后，在所有全局‘debug directory’里的和可执行文件绝对目录名字一样的子目录里搜索。 对于‘build-id’方式，GDB在每一个全局‘debug directory’里名为‘build-id’的子目录中搜索名为‘nn/nnnnnnnn.debug’的文件（CentOS中该文件是一个符号链接）。 准备好了以上文件后，还需要最后一个很重要的文件，该文件定义了调试需要用到的GDB命令。该文件位于 ./usr/lib/debug/usr/lib64/libpython2.7.so.1.0.debug-gdb.py 将该文件重命名为python-gdb.py，并和其他文件放一起。现在gdb目录的结构是这样的： gdb data-directory usr python-gdb.py 启动gdb后需要import python-gdb.py，修改PYTHONPATH export PYTHONPATH=&quot;${PYTHONPATH}:${HOME}/gdb&quot; 然后启动gdb ./gdb --data-directory=./data-directory 设置debug文件查找目录，告诉GDB去准备好的解包目录找调试信息 set debug-file-directory ./usr/lib/debug/ 载入python模块，载入自定义GDB命令 python import python-gdb 然后attach到需要调试的进程就可以了 attach pid Ref MiniDebugInfo Building Python Statically how-i-can-static-build-gdb-from-source How do I extract the contents of an rpm? Debugging Information in Separate Files]]></content>
  </entry>
  <entry>
    <title><![CDATA[MongoDB Data Migration]]></title>
    <url>%2FMongoDB-Data-Migrate%2F</url>
    <content type="text"><![CDATA[MongoDB使用json和bson（binary json）格式存储信息。json使用起来非常方便，但是其并不支持bson里的所有数据类型。如果使用json备份数据就会有信息丢失。 备份和恢复MongoDB数据库可能会消耗大量的CPU、内存和磁盘资源，最好是在非高峰期进行。 需要考虑数据一致性的问题。 一致性（Consistency）一致性是指事务使得系统从一个一致的状态转换到另一个一致状态。事务的一致性决定了一个系统设计和实现的复杂度。事务可以不同程度的一致性：强一致性：读操作可以立即读到提交的更新操作。弱一致性：提交的更新操作，不一定立即会被读操作读到，此种情况会存在一个不一致窗口，指的是读操作可以读到最新值的一段时间。最终一致性：是弱一致性的特例。事务更新一份数据，最终一致性保证在没有其他事务更新同样的值的话，最终所有的事务都会读到之前事务更新的最新值。如果没有错误发生，不一致窗口的大小依赖于：通信延迟，系统负载等。 其他一致性变体还有：单调一致性：如果一个进程已经读到一个值，那么后续不会读到更早的值。会话一致性：保证客户端和服务器交互的会话过程中，读操作可以读到更新操作后的最新值。 copydb &amp; clone Concurrency copydb and clone do not produce point-in-time snapshots of the source database. Write traffic to the source or destination database during the copy process will result in divergent data sets. copydb does not lock the destination server during its operation, so the copy will occasionally yield to allow other operations to complete. clone does not snapshot the database. If any clients update the database you’re copying at any point during the clone operation, the resulting database may be inconsistent. 有数据一致性的问题。 mongodump &amp; mongorestore 以bson格式保存，恢复完成后必须重建索引。 对于小型的数据库是简单高效的备份恢复工具，但是对于大型系统不是一个理想的选择。 mongodump会显著影响MongoDB的性能。如果数据库大小超过系统内存，有可能出现内存不足和页错误。 without –oplog， if there are write operations during the dump operation, the dump will not reflect a single moment in time. Changes made to the database during the update process can affect the output of the backup. 使用oplog可以保证数据一致性 master &amp; slaveslave把master的数据保存在了local.sources collection。迁移完成后需要进一步处理。本质上也是利用了oplog, 可以保证数据一致性。 Replication Set相当于增强的master slave模式，带有failover机制，多个节点选取一个为主节点，其他为次节点，主节点可读写，次节点只能读，当主节点挂掉会自动选举新的主节点，方便加入新的次节点，扩展数据库系统，本质上也是利用了oplog, 可以保证数据一致性。 总结copydb &amp; clone 有数据一致性问题不考虑使用。master &amp; slave模式，slave后续还需要进一步操作，并且官方已不推荐部署此模式。Replication Set提供了很多高级特性，是官方现在推荐的部署模式。处理数据迁移时，master &amp; slave和Replication Set比较接近，前者需要进一步处理，后者配置略麻烦，都相当于自动mongodump + oplogreplay，在数据不是很大的情况下，可以配置一个单节点的Replication Set配合mongodump with oplog比较合适。 oplog有一个非常重要的特性——幂等性（idempotent）。即对一个数据集合，使用oplog中记录的操作重放时，无论被重放多少次，其结果是一样的。举例来说，如果oplog中记录的是一个插入操作，并不会因为你重放了两次，数据库中就得到两条相同的记录。 还有一种方法是使用文件系统提供的快照功能，比如LVM。 Ref How To Back Up, Restore, and Migrate a MongoDB Database on Ubuntu 14.04 Migrating MongoDB instances with no down-time MongoDB Clone MongoDB Copydb MongoDB Replication Master Slave Replication]]></content>
  </entry>
  <entry>
    <title><![CDATA[Inverted Index in the Luence]]></title>
    <url>%2FInverted-Index-in-the-Luence%2F</url>
    <content type="text"><![CDATA[假设有一本书，前面有目录，中间是正文，最后会有附录。有这样一种附录，它会把正文中出现的关键名词列出来，并附上正文中出现的页码。当我们想找某个术语的相关信息时，去附录里查正文出现的页码显然方便些。目录类似于我们常规的索引，一个章节/文件里有什么内容；附录保存的信息和它相反，这个信息出现在哪些章节/文件里。我们称附录这种数据的索引方式叫做“Inverted Index”，中文一般翻译为“倒排索引”。 这种数据结构是搜索程序的核心数据结构，它本身并不复杂，很容易理解，网上的解释也很多，具体可以参考维基百科。因为最近接触ElasticSearch，比较好奇它的倒排索引是怎么实现的。 ElasticSearch是一个基于Lucene的全文搜索引擎。Lucene提供了搜索的核心功能，维护倒排索引是其中的功能之一。 Luence的倒排索引是由许多子片段组成的。每一个子片段是一个完整的能独立工作的子索引，由几个二进制文件组成。当有新的文档需要被索引时，会新建子索引，当有文档被删除时，并不会删除对应的索引数据，而是做标记，记录这些数据已经失效了。子索引数据的合并和删除会在某一时间统一执行。 Luence里对数据的基本定义是这样的： The fundamental concepts in Lucene are index, document, field and term. An index contains a sequence of documents. A document is a sequence of fields. A field is a named sequence of terms. A term is a sequence of bytes. The same sequence of bytes in two different fields is considered a different term. Thus terms are represented as a pair: the string naming the field, and the bytes within the field. 所有这些信息都被保存到几个二进制文件里。Lucene为了减少数据的大小，对数据采用了bit packing的压缩方法。举个例子，一个字节可以表示0～127的无符号数，但是如果我们的数据确定最大不会超过63，那就可以只用4位来表示，这样就省去了一半的空间。类似的方法还有ZigZag编码。这部分实现对应代码org.apache.lucene.util.packed。 为了提高搜索效率，Lucene底层查找Term Dictionary(记录了term在文档中的位置信息)时使用了跳跃链表的数据结构。它相对于平衡二叉树简单，但是也有不错的性能，Redies中也有用到。 Ref What is in a Lucene index Exploring Solr Internals : The Lucene Inverted Index Lucene file format How fast is bit packing 跳跃链表 ZigZag in ProtoBuffers]]></content>
  </entry>
  <entry>
    <title><![CDATA[Valgrind工作原理简介]]></title>
    <url>%2Fhow-valgrind-work%2F</url>
    <content type="text"><![CDATA[Valgrind是一款用于构建内存调试、内存泄露检测以及性能分析的软件开发框架，它是一个由来自全世界的开发者组织合作开发得到的开源软件。他的初始作者Julian Seward在2006年获得第二届Google-O’Reilly开源代码奖。Valgrind这个名字取自北欧神话中英灵殿的入口。 官方首页上这个骑士与恶龙的绘画由伦敦画家 Rupert Lees 创作。灵感来自于神话传说 St George and the Dragon 。某日，圣乔治到利比亚去，当地沼泽中的一只恶龙（一说鳄鱼）在水泉旁边筑巢，这水泉是Silene城唯一的水源，市民为了取水，每天都要把两头绵羊献祭给恶龙。 到后来，绵羊都吃完了，只好用活人来替代，每天抽签决定何人应选派作牺牲。 有一天，国王的女儿被抽中，国王也没有办法，悲痛欲绝。当少女走近，正要被恶龙吞吃时，圣乔治在这时赶到，提起利矛对抗恶龙，并用腰带把它束缚住，牵到城里当众杀死，救出了公主。 在软件开发过程中我们有时需要追踪程序执行、诊断错误、衡量性能等需求。如果有源代码的话可以添加相应代码，没有源代码时这个方法就行不通了。前者可以对源代码分析，后者只能分析二进制了。我的理解是，dynamic Binary Instrumentation (DBI)强调对二进制执行文件的追踪与信息收集，dynamic Binary Analysis (DBA)强调对收集信息的分析，valgrind是一个DBI框架，使用它可以很方便构建一个DBA工具。 Valgrind作者认为当时的DBI框架都把注意力放在了性能检测上，对程序的质量(原文capabilities)并没有太注意。所以设计了一个新的DBI框架，目标是可以使用它开发重型DBA工具。 Valgrind的重点是shadow values技术，该技术要求对所有的寄存器和使用到的内存做shadow（自己维护一份）。因此使用valgrind开发出来的工具一般跑的都比较慢。为了实现shadow values需要框架实现以下4个部分： Shadow State 提供 shadow registers (例如 integer、FP、SIMD) 提供 shadow memory 读写操作 9个具体功能: instrument read/write instructions instrument read/write system calls Allocation and deallocation operations instrument start-up allocations instrument system call (de)allocations instrument stack (de)allocations instrument heap (de)allocations Transparent execution, but with extra output extra output 核心思想就是要把寄存器和内存中的东西自己维护一份，并且在任何情况下都可以安全正确地使用，同时记录程序的所有操作，在不影响程序执行结果前提下，输出有用的信息。使用shadow values技术的DBI框架都使用不同方式实现了上述全部功能或部分功能。 valgrind结构上分为core和tool，不同的tool具有不同的功能。比较特别的是，valgrind tool都包含core的静态链接，虽然有点浪费空间，但可以简化某些事情。当我们在调用valgrind时，实际上启动的只是一个解析命令参数的启动器，由这个启动器启动具体的tool。 为了实现上述功能，valgrind会利用dynamic binary re-compilation把测试程序（client程序）的机器码解析到VEX中间语言。VEX IR是valgrind开发者专门设计给DBI使用的中间语言，是一种RISC like的语言。目前VEX IR从valgrind分离出去成libVEX了。libVEX采用execution-driven的方式用just-in-time技术动态地把机器码转换为IR，如果发生了某些tool感兴趣的事件，就会hook tool的函数，tool会插入一些分析代码，再把这些代码转换为机器码，存储到code cache中，以便再需要的时候执行。 Machine Code --&gt; IR --&gt; IR --&gt; Machine Code ^ ^ ^ | | | translate | | | | instrument | | translate valgrind启动后，core、tool和client都在一个进程中，共用一个地址空间。core首先会初始化必要的组件，然后载入client，建立client的stack，完成后会要求tool初始化自己，tool完成剩余部分的初始化，这样tool就具有了控制权，开始转换client程式。从某种意义上说，valgrind执行的都是加工后的client程序的代码。 DBI framework 有两种基本的方式可以表示code和进行 instrumentation： disassemble-and-resynthesise (D&amp;R)。Valgrind 使用这种把machine code先转成IR，IR会通过加入更IR来instrument。IR最后转回machine code执行，原本的code对guest state的所有影响都必须明确地转成IR，因为最后执行的是纯粹由IR转成的machine code。 copy-and-annotate (C&amp;A)。instructions会被逐字地复制(除了一些 control flow 改变)每个instruction都加上注解描述其影响(annotate)，利用这些描述来帮助做instrumentation通过给每条指令添加一个额外的data structure (DynamoRIO)通过提供相应的获取指令相关信息的API (Intel Pin)这些添加的注解可以指导进行相应的instrument，并且不影响原来的native code的执行效果。 Ref 本文主要参考了官方的研究论文（ http://valgrind.org/docs/pubs.html ）。]]></content>
  </entry>
</search>
