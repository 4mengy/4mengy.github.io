<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Deadlock caused by forking a multithreaded process]]></title>
    <url>%2Fdeadlock-caused-by-forking-a-multithreaded-process%2F</url>
    <content type="text"><![CDATA[最近生产环境的一个模块突然卡住了，模块的作者已经去其他组了，没办法只能自己查问题了🤣。 该模块使用python的logging标准库记录日志，自定义kafka作为handler，日志通过kafka发送到Elasticsearch，然后就是ELK日志处理了。 模块为了提高吞吐，内部使用了多进程，问题恰恰就出现在了这里。 使用GDB attach到卡死的进程，执行py-bt命令，输出如下： Traceback (most recent call first): File “/usr/lib64/python2.7/multiprocessing/forking.py”, line 135, in poll pid, sts = os.waitpid(self.pid, flag) File “/usr/lib64/python2.7/multiprocessing/forking.py”, line 154, in wait return self.poll(0) File “/usr/lib64/python2.7/multiprocessing/process.py”, line 145, in join res = self._popen.wait(timeout) File “parser/parser_main.py”, line 76, in data_parse process.join() 主进程里使用multiprocessing模块创建多进程执行任务，这里主进程在等待子进程执行结束。 阅读multiprocessing模块的源代码，可以发现其创建子进程时调用了os.fork()函数，该函数是python os模块对系统调用fork的封装。 子进程的输出： Traceback (most recent call first): Waiting for the GIL File “sdk/python-lib/kafka/producer/record_accumulator.py”, line 223, in append with self._tp_locks[tp]: File “sdk/python-lib/kafka/producer/kafka.py”, line 516, in send self.config[‘max_block_ms’]) File “sdk/python-lib/elk_logging/handlers/kafka.py”, line 37, in emit self.format(record).encode(‘utf-8’)) elk_logging模块自定义了kafka handler，模块导入时通过logging.config.dictConfig()配置logging模块。kafak发送消息时获取不到锁self._tp_lock[tp]，然后一直等待。 在Google上搜索“kafka self._tp_lock deadlock”没有发现有价值的信息，基本排除是kafka库的bug，如果是kafka库的bug，网上不太可能一点线索也没有。 前文我们提到这里使用了fork创建子进程，当内核执行fork时，生成的子进程是父进程的副本，子进程获得父进程的数据空间、堆和栈的副本（这里是子进程自己拥有的副本，并不与父进程共享这些存储空间。很多实现使用了写时复制技术，一开始这些区域被父子进程共享，如果任意一个进程修改这些区域，则内核只为修改的那块内存制作一个副本，通常是虚拟内存的一页）。 在Linux系统中调用fork，创建的子进程中只有一个线程——父进程中调用fork函数的线程。 Note the following further points: The child process is created with a single thread—the one that called fork(). The entire virtual address space of the parent is replicated in the child, including the states of mutexes, condition variables, and other pthreads objects; the use of pthread_atfork(3) may be helpful for dealing with problems that this can cause. 也不是所有的系统都是这样，Solaris系统libthread库提供的fork函数会“复制“所有线程。 Solaris libthread supports both fork() and fork1(). The fork() call has “fork-all” semantics–it duplicates everything in the process, including threads and LWPs, creating a true clone of the parent. The fork1() call creates a clone that has only one thread; the process state and address space are duplicated, but only the calling thread is cloned. POSIX libpthread supports only fork(), which has the same semantics as fork1() in Solaris threads. POSIX标准定义的行为是fork只“复制”一个线程。 那么这些与上面的死锁有什么关系？python标准库logging模块在创建logger时会调用到以下代码，可以看到一个logger只会实例化一次，对应的handler和formatter等也只会实例化一次。 logging/__init__.py manager.getLogger(name)123456789101112131415161718_acquireLock()try: if name in self.loggerDict: rv = self.loggerDict[name] if isinstance(rv, PlaceHolder): ph = rv rv = (self.loggerClass or _loggerClass)(name) rv.manager = self self.loggerDict[name] = rv self._fixupChildren(ph, rv) self._fixupParents(rv) else: rv = (self.loggerClass or _loggerClass)(name) rv.manager = self self.loggerDict[name] = rv self._fixupParents(rv)finally: _releaseLock() 如果自定义了handler，配置logging模块时会导入指定的handler class，并实例化。 logging/config.py DictConfigurator.configure_handler()123456789101112131415161718if '()' in config: c = config.pop('()') if not callable(c): c = self.resolve(c) factory = celse: cname = config.pop('class') klass = self.resolve(cname) factory = klassprops = config.pop('.', None)kwargs = dict([(k, config[k]) for k in config if valid_ident(k)])try: result = factory(**kwargs)except TypeError as te: if "'stream'" not in str(te): raise kwargs['strm'] = kwargs.pop('stream') result = factory(**kwargs) 父进程和fork出来的子进程使用的时相同的对象实例，相同的logger，相同的kafka handler。 kafka producer发送消息是多线程异步的。KafkaProducer实例化时会启动一个消息发送线程。有消息过来后，先通过RecordAccumulator类实例将消息追加到内部list，然后唤醒self._sender发送线程，发送线程通过RecordAccumulator类实例检查哪些可以发送，整个过程RecordAccumulator类实例维护了topic+partition相关的锁。1234567891011121314def __init__(self, **configs): ... self._accumulator = RecordAccumulator(message_version=message_version, metrics=self._metrics, **self.config) self._sender = Sender(client, self._metadata, self._accumulator, self._metrics, guarantee_message_order=guarantee_message_order, **self.config) self._sender.daemon = True self._sender.start() self._closed = False self._cleanup = self._cleanup_factory() atexit.register(self._cleanup) log.debug("Kafka producer started") 如果fork时RecordAccumulator类实例中某个topic+partition的锁还没有释放，子进程中该锁永远不会被释放，因为没有了发送线程，如果子进程中恰好向同一个topic和同一个partition发送消息就会死锁。 由于子进程不会常驻在后台，所以满足上述条件还是比较难的，从表现上来看，5个月里发生了一次死锁。 后记避免这种问题，最好的办法就是不要fork一个多线程进程。 python的标准库logging模块曾经因为类似的原因出过问题。现在官方文档已经指出： Note that safely forking a multithreaded process is problematic. Google甚至还创建过一个项目python-atfork，模仿pthread_atfork，可以注册函数，保正多线程的情况下可以安全的fork。当然，该项目已经被废弃了。 如果使用较新的python版本（3.5+ ？）可以指定multiprocessing模块创建进程的方式，可以尝试spawn方式。 Ref UNIX环境高级编程 Linux fork manual Special Issues for fork() and Solaris Threads python multiprocessing doc Safe to call multiprocessing from a thread in Python？ reentrancy, thread safe, asynchronous signal safe, interrupt safe and cancellation safe Threads and fork(): think twice before mixing them]]></content>
  </entry>
  <entry>
    <title><![CDATA[Debug Python with GDB in off-line CentOS]]></title>
    <url>%2Fdebug-python-with-gdb-in-offline-centos%2F</url>
    <content type="text"><![CDATA[GDB不仅可以调试C/C++程序，它还可以调试Python等程序，是非常重要和有力的开发工具之一。大部分情况下，都可以直接安装发行版提供的gdb来调试程序，但是，某些情况下需要自己编译，例如，机器无法上网；甚至没有权限修改系统，无法安装软件。下面以CentOS为例，介绍如何在离线并且没有全部权限的情况下使用GDB调试Python程序。 首先，需要在相同系统版本可以上网的机器上编译GDB wget -c http://ftp.gnu.org/gnu/gdb/gdb-8.0.1.tar.gz tar xvf gdb-8.0.1.tar.gz cd gdb-8.0.1.tar.gz mkdir build cd build ../configure --enable-static --with-python --disable-interprocess-agent --with-lzma make 需要拿到离线机器使用，所以这里使用‘–enable-static –disable-interprocess-agent’，期望获得一个静态链接的二进制文件。通过‘–with-python’启用了python调试功能。‘–with-lzma’启用了lzma支持，由于某些发行版分发的二进制文件使用了‘MiniDebugInfo’特性：在二进制文件中存在一个名为‘gnu_debugdata’的特殊区域，该区域保存了经过lzma压缩的额外符号信息。 如果在configure过程中报错，一般都是缺少一些依赖包，根据提示进行安装就可以了。 编译完成后，需要的文件都在build/gdb目录内，将他们复制到另一个目录 mkdir -p ~/gdb/data-directory cd gdb cp gdb ~/gdb/gdb cp -r data-directory/* ~/gdb/data-directory cd ~/gdb chmod +x ./gdb data-directory目录内保存了gdb需要使用的数据文件。现在就可以使用这个gdb在离线系统上简单调试程序了 ./gdb --data-directory=./data-directory your-program 但是当进入到一些库函数时，会出现错误，因为系统上没有相应的调试信息文件。需要手动部署这些文件，以调试Python为例，首先获取系统中python包的版本 yum list installed | grep ^python.x86_64 假如得到的信息是‘python.x86_64 2.7.5-58.el7’，去debuginfo.centos.org下载相应的调试信息文件，例如，’python-debuginfo-2.7.5-58.el7.x86_64.rpm‘。 将该文件解包 rpm2cpio python-debuginfo-2.7.5-58.el7.x86_64.rpm | cpio -idmv 正常情况下，该文件是通过rpm安装到系统里的，安装后的结构和直接解压出来的结构可能不同，系统目录： ls -l /usr/lib/debug/ drwxr-xr-x. 3 root root 64 11月 5 2016 . dr-xr-xr-x. 44 root root 4096 10月 28 21:38 .. lrwxrwxrwx. 1 root root 7 10月 28 21:21 bin -&gt; usr/bin lrwxrwxrwx. 1 root root 7 10月 28 21:21 lib -&gt; usr/lib lrwxrwxrwx. 1 root root 9 10月 28 21:21 lib64 -&gt; usr/lib64 lrwxrwxrwx. 1 root root 8 10月 28 21:21 sbin -&gt; usr/sbin drwxr-xr-x. 6 root root 65 10月 28 21:21 usr 解包目录： ls -al ./usr/lib/debug/ drwxr-xr-x. 5 buildbot buildbot 46 10月 29 23:26 . drwxrwxr-x. 3 buildbot buildbot 19 10月 29 23:26 .. drwxr-xr-x. 114 buildbot buildbot 4096 10月 29 23:26 .build-id drwxr-xr-x. 2 buildbot buildbot 40 10月 29 23:26 .dwz drwxr-xr-x. 4 buildbot buildbot 30 10月 29 23:26 usr 系统目录中的符号链接，解包目录里是没有的，需要手动加上。解包目录里的‘.build-id’文件夹是非常重要的文件夹。 GDB允许将调试信息和可执行代码分开存放，因为一般情况，调试信息比可执行代码大很多，普通用户不需要调试信息。分开存放很好的解决了此问题。 GDB支持两种指定调试信息文件的方法： 可执行文件包含一个‘debug link’，其指定了调试文件的名字。通常调试信息文件的名字是‘executable.debug’，其中，‘executable’是可执行文件的名字（不包括路径）。并且，‘debug link’还包含了可执行文件的CRC32值，GDB可以利用该值判断可执行文件和调试信息文件是否来自相同的build。 可执行文件包含一个特殊的字符串‘build-id’，该字符串同时也存在于调试信息文件中。调试信息文件的名字可以通过‘build-id’计算出来。 对于以上两种不同的方式，GDB采用两种方法查找调试信息文件： 对于‘debug link’方式，GDB先在可执行文件所在目录搜索指定名字的调试信息文件，然后是可执行文件所在目录中名为‘.debug’的子目录里搜索，最后，在所有全局‘debug directory’里的和可执行文件绝对目录名字一样的子目录里搜索。 对于‘build-id’方式，GDB在每一个全局‘debug directory’里名为‘build-id’的子目录中搜索名为‘nn/nnnnnnnn.debug’的文件（CentOS中该文件是一个符号链接）。 准备好了以上文件后，还需要最后一个很重要的文件，该文件定义了调试需要用到的GDB命令。该文件位于 ./usr/lib/debug/usr/lib64/libpython2.7.so.1.0.debug-gdb.py 将该文件重命名为python-gdb.py，并和其他文件放一起。现在gdb目录的结构是这样的： gdb data-directory usr python-gdb.py 启动gdb后需要import python-gdb.py，修改PYTHONPATH export PYTHONPATH=&quot;${PYTHONPATH}:${HOME}/gdb&quot; 然后启动gdb ./gdb --data-directory=./data-directory 设置debug文件查找目录，告诉GDB去准备好的解包目录找调试信息 set debug-file-directory ./usr/lib/debug/ 载入python模块，载入自定义GDB命令 python import python-gdb 然后attach到需要调试的进程就可以了 attach pid Ref MiniDebugInfo Building Python Statically how-i-can-static-build-gdb-from-source How do I extract the contents of an rpm? Debugging Information in Separate Files]]></content>
  </entry>
  <entry>
    <title><![CDATA[MongoDB Data Migration]]></title>
    <url>%2FMongoDB-Data-Migrate%2F</url>
    <content type="text"><![CDATA[MongoDB使用json和bson（binary json）格式存储信息。json使用起来非常方便，但是其并不支持bson里的所有数据类型。如果使用json备份数据就会有信息丢失。 备份和恢复MongoDB数据库可能会消耗大量的CPU、内存和磁盘资源，最好是在非高峰期进行。 需要考虑数据一致性的问题。 一致性（Consistency）一致性是指事务使得系统从一个一致的状态转换到另一个一致状态。事务的一致性决定了一个系统设计和实现的复杂度。事务可以不同程度的一致性：强一致性：读操作可以立即读到提交的更新操作。弱一致性：提交的更新操作，不一定立即会被读操作读到，此种情况会存在一个不一致窗口，指的是读操作可以读到最新值的一段时间。最终一致性：是弱一致性的特例。事务更新一份数据，最终一致性保证在没有其他事务更新同样的值的话，最终所有的事务都会读到之前事务更新的最新值。如果没有错误发生，不一致窗口的大小依赖于：通信延迟，系统负载等。 其他一致性变体还有：单调一致性：如果一个进程已经读到一个值，那么后续不会读到更早的值。会话一致性：保证客户端和服务器交互的会话过程中，读操作可以读到更新操作后的最新值。 copydb &amp; clone Concurrency copydb and clone do not produce point-in-time snapshots of the source database. Write traffic to the source or destination database during the copy process will result in divergent data sets. copydb does not lock the destination server during its operation, so the copy will occasionally yield to allow other operations to complete. clone does not snapshot the database. If any clients update the database you’re copying at any point during the clone operation, the resulting database may be inconsistent. 有数据一致性的问题。 mongodump &amp; mongorestore 以bson格式保存，恢复完成后必须重建索引。 对于小型的数据库是简单高效的备份恢复工具，但是对于大型系统不是一个理想的选择。 mongodump会显著影响MongoDB的性能。如果数据库大小超过系统内存，有可能出现内存不足和页错误。 without –oplog， if there are write operations during the dump operation, the dump will not reflect a single moment in time. Changes made to the database during the update process can affect the output of the backup. 使用oplog可以保证数据一致性 master &amp; slaveslave把master的数据保存在了local.sources collection。迁移完成后需要进一步处理。本质上也是利用了oplog, 可以保证数据一致性。 Replication Set相当于增强的master slave模式，带有failover机制，多个节点选取一个为主节点，其他为次节点，主节点可读写，次节点只能读，当主节点挂掉会自动选举新的主节点，方便加入新的次节点，扩展数据库系统，本质上也是利用了oplog, 可以保证数据一致性。 总结copydb &amp; clone 有数据一致性问题不考虑使用。master &amp; slave模式，slave后续还需要进一步操作，并且官方已不推荐部署此模式。Replication Set提供了很多高级特性，是官方现在推荐的部署模式。处理数据迁移时，master &amp; slave和Replication Set比较接近，前者需要进一步处理，后者配置略麻烦，都相当于自动mongodump + oplogreplay，在数据不是很大的情况下，可以配置一个单节点的Replication Set配合mongodump with oplog比较合适。 oplog有一个非常重要的特性——幂等性（idempotent）。即对一个数据集合，使用oplog中记录的操作重放时，无论被重放多少次，其结果是一样的。举例来说，如果oplog中记录的是一个插入操作，并不会因为你重放了两次，数据库中就得到两条相同的记录。 还有一种方法是使用文件系统提供的快照功能，比如LVM。 RefHow To Back Up, Restore, and Migrate a MongoDB Database on Ubuntu 14.04 Migrating MongoDB instances with no down-time MongoDB Clone MongoDB Copydb MongoDB Replication Master Slave Replication]]></content>
  </entry>
  <entry>
    <title><![CDATA[Inverted Index in the Luence]]></title>
    <url>%2FInverted-Index-in-the-Luence%2F</url>
    <content type="text"><![CDATA[假设有一本书，前面有目录，中间是正文，最后会有附录。有这样一种附录，它会把正文中出现的关键名词列出来，并附上正文中出现的页码。当我们想找某个术语的相关信息时，去附录里查正文出现的页码显然方便些。目录类似于我们常规的索引，一个章节/文件里有什么内容；附录保存的信息和它相反，这个信息出现在哪些章节/文件里。我们称附录这种数据的索引方式叫做“Inverted Index”，中文一般翻译为“倒排索引”。 这种数据结构是搜索程序的核心数据结构，它本身并不复杂，很容易理解，网上的解释也很多，具体可以参考维基百科。因为最近接触ElasticSearch，比较好奇它的倒排索引是怎么实现的。 ElasticSearch是一个基于Lucene的全文搜索引擎。Lucene提供了搜索的核心功能，维护倒排索引是其中的功能之一。 Luence的倒排索引是由许多子片段组成的。每一个子片段是一个完整的能独立工作的子索引，由几个二进制文件组成。当有新的文档需要被索引时，会新建子索引，当有文档被删除时，并不会删除对应的索引数据，而是做标记，记录这些数据已经失效了。子索引数据的合并和删除会在某一时间统一执行。 Luence里对数据的基本定义是这样的： The fundamental concepts in Lucene are index, document, field and term. An index contains a sequence of documents. A document is a sequence of fields. A field is a named sequence of terms. A term is a sequence of bytes. The same sequence of bytes in two different fields is considered a different term. Thus terms are represented as a pair: the string naming the field, and the bytes within the field. 所有这些信息都被保存到几个二进制文件里。Lucene为了减少数据的大小，对数据采用了bit packing的压缩方法。举个例子，一个字节可以表示0～127的无符号数，但是如果我们的数据确定最大不会超过63，那就可以只用4位来表示，这样就省去了一半的空间。类似的方法还有ZigZag编码。这部分实现对应代码org.apache.lucene.util.packed。 为了提高搜索效率，Lucene底层查找Term Dictionary(记录了term在文档中的位置信息)时使用了跳跃链表的数据结构。它相对于平衡二叉树简单，但是也有不错的性能，Redies中也有用到。 RefWhat is in a Lucene index Exploring Solr Internals : The Lucene Inverted Index Lucene file format How fast is bit packing 跳跃链表 ZigZag in ProtoBuffers]]></content>
  </entry>
  <entry>
    <title><![CDATA[Valgrind工作原理简介]]></title>
    <url>%2Fhow-valgrind-work%2F</url>
    <content type="text"><![CDATA[Valgrind是一款用于构建内存调试、内存泄露检测以及性能分析的软件开发框架，它是一个由来自全世界的开发者组织合作开发得到的开源软件。他的初始作者Julian Seward在2006年获得第二届Google-O’Reilly开源代码奖。Valgrind这个名字取自北欧神话中英灵殿的入口。 官方首页上这个骑士与恶龙的绘画由伦敦画家 Rupert Lees 创作。灵感来自于神话传说 St George and the Dragon 。某日，圣乔治到利比亚去，当地沼泽中的一只恶龙（一说鳄鱼）在水泉旁边筑巢，这水泉是Silene城唯一的水源，市民为了取水，每天都要把两头绵羊献祭给恶龙。 到后来，绵羊都吃完了，只好用活人来替代，每天抽签决定何人应选派作牺牲。 有一天，国王的女儿被抽中，国王也没有办法，悲痛欲绝。当少女走近，正要被恶龙吞吃时，圣乔治在这时赶到，提起利矛对抗恶龙，并用腰带把它束缚住，牵到城里当众杀死，救出了公主。 在软件开发过程中我们有时需要追踪程序执行、诊断错误、衡量性能等需求。如果有源代码的话可以添加相应代码，没有源代码时这个方法就行不通了。前者可以对源代码分析，后者只能分析二进制了。我的理解是，dynamic Binary Instrumentation (DBI)强调对二进制执行文件的追踪与信息收集，dynamic Binary Analysis (DBA)强调对收集信息的分析，valgrind是一个DBI框架，使用它可以很方便构建一个DBA工具。 Valgrind作者认为当时的DBI框架都把注意力放在了性能检测上，对程序的质量(原文capabilities)并没有太注意。所以设计了一个新的DBI框架，目标是可以使用它开发重型DBA工具。 Valgrind的重点是shadow values技术，该技术要求对所有的寄存器和使用到的内存做shadow（自己维护一份）。因此使用valgrind开发出来的工具一般跑的都比较慢。为了实现shadow values需要框架实现以下4个部分： Shadow State 提供 shadow registers (例如 integer、FP、SIMD) 提供 shadow memory 读写操作 9个具体功能: instrument read/write instructions instrument read/write system calls Allocation and deallocation operations instrument start-up allocations instrument system call (de)allocations instrument stack (de)allocations instrument heap (de)allocations Transparent execution, but with extra output extra output 核心思想就是要把寄存器和内存中的东西自己维护一份，并且在任何情况下都可以安全正确地使用，同时记录程序的所有操作，在不影响程序执行结果前提下，输出有用的信息。使用shadow values技术的DBI框架都使用不同方式实现了上述全部功能或部分功能。 valgrind结构上分为core和tool，不同的tool具有不同的功能。比较特别的是，valgrind tool都包含core的静态链接，虽然有点浪费空间，但可以简化某些事情。当我们在调用valgrind时，实际上启动的只是一个解析命令参数的启动器，由这个启动器启动具体的tool。 为了实现上述功能，valgrind会利用dynamic binary re-compilation把测试程序（client程序）的机器码解析到VEX中间语言。VEX IR是valgrind开发者专门设计给DBI使用的中间语言，是一种RISC like的语言。目前VEX IR从valgrind分离出去成libVEX了。libVEX采用execution-driven的方式用just-in-time技术动态地把机器码转换为IR，如果发生了某些tool感兴趣的事件，就会hook tool的函数，tool会插入一些分析代码，再把这些代码转换为机器码，存储到code cache中，以便再需要的时候执行。 Machine Code --&gt; IR --&gt; IR --&gt; Machine Code ^ ^ ^ | | | translate | | | | instrument | | translate valgrind启动后，core、tool和client都在一个进程中，共用一个地址空间。core首先会初始化必要的组件，然后载入client，建立client的stack，完成后会要求tool初始化自己，tool完成剩余部分的初始化，这样tool就具有了控制权，开始转换client程式。从某种意义上说，valgrind执行的都是加工后的client程序的代码。 DBI framework 有两种基本的方式可以表示code和进行 instrumentation： disassemble-and-resynthesise (D&amp;R)。Valgrind 使用这种把machine code先转成IR，IR会通过加入更IR来instrument。IR最后转回machine code执行，原本的code对guest state的所有影响都必须明确地转成IR，因为最后执行的是纯粹由IR转成的machine code。 copy-and-annotate (C&amp;A)。instructions会被逐字地复制(除了一些 control flow 改变)每个instruction都加上注解描述其影响(annotate)，利用这些描述来帮助做instrumentation通过给每条指令添加一个额外的data structure (DynamoRIO)通过提供相应的获取指令相关信息的API (Intel Pin)这些添加的注解可以指导进行相应的instrument，并且不影响原来的native code的执行效果。 Ref本文主要参考了官方的研究论文（ http://valgrind.org/docs/pubs.html ）。]]></content>
  </entry>
</search>
